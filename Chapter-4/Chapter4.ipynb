{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src='images/Neuron.svg' width=60%\\>\n",
    "</center>\n",
    "\n",
    "[Fuente Wikipedia](https://commons.wikimedia.org/w/index.php?title=File:Neuron.svg&oldid=343028396)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una red neuronal es una estructura compuesta por **nodos** o **unidades** que se encuentran interconectados. La potencia de la interconexión entre los nodos se evalúa por medio un valor de **peso**. Si la suma ponderada de todas las conexiones al **nodo** o **neurona** es mayor que un **valor umbral**, decimos que la neurona se **activa**. La función matemática aplicada a la suma ponderada se denomina **función de activación**. \n",
    "\n",
    "Se denomina **Modelo de Perceptrón** a una red neuronal con una sola salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"174pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 174.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-257 170,-257 170,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"8,-8 8,-245 60,-245 60,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"106,-89 106,-164 158,-164 158,-89 106,-89\"/>\n",
       "<text text-anchor=\"middle\" x=\"132\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output</text>\n",
       "</g>\n",
       "<!-- x[0] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-196\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-192.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[0]</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"132\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"132\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M48.041,-184.3947C64.2693,-170.9815 91.1723,-148.7454 110.144,-133.0647\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.5069,-135.6525 117.985,-126.5838 108.0473,-130.2569 112.5069,-135.6525\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[0]</text>\n",
       "</g>\n",
       "<!-- x[1] -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-142\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-138.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[1]</text>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.5204,-137.1729C66.2924,-133.1031 87.6838,-127.2096 104.6999,-122.5214\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.9201,-125.8158 114.6312,-119.7853 104.0608,-119.0672 105.9201,-125.8158\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[1]</text>\n",
       "</g>\n",
       "<!-- x[2] -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-88\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[2]</text>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.5204,-92.8271C66.2924,-96.8969 87.6838,-102.7904 104.6999,-107.4786\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.0608,-110.9328 114.6312,-110.2147 105.9201,-104.1842 104.0608,-110.9328\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-107.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[2]</text>\n",
       "</g>\n",
       "<!-- x[3] -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x[3]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-34\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[3]</text>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;y -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M48.041,-45.6053C64.2693,-59.0185 91.1723,-81.2546 110.144,-96.9353\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.0473,-99.7431 117.985,-103.4162 112.5069,-94.3475 108.0473,-99.7431\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-82.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[3]</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f4b80493630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_logistic_regression_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sea $\\mathbf{X}$ el espacio de entrada que contiene $N$ muestras de datos. Cada muestra está descrita por $d$ características o **features**. Sea $\\mathcal{Y} = \\{-1, +1\\}$ el espacio de salida binario. El perceptrón queda definido por:\n",
    "\n",
    "\n",
    "    \n",
    "$h(\\mathbf{x}) = sign \\left( \\sum_{i=1}^{d} \\left( w_ix_i \\right) + b \\right)$\n",
    "\n",
    "Donde **sign** es la función signo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sea $\\mathbf{w} = \\{w_{0},w_{1}, w_{2}, \\dots, w_{d} \\}^T$ el vector de pesos; en donde $w_{0}=b$ y sea $\\mathbf{x} = \\{x_{0},x_{1}, x_{2}, \\dots, x_{d} \\}^T $ con $w_{0}=1$, entonces la expresión para el Perceptron se puede reescribir:\n",
    "\n",
    "$h(\\mathbf{x}) = sign \\left(  \\mathbf{w}^T \\mathbf{x} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo de aprendizaje del perceptrón\n",
    "Para realizar el entrenamiento es preciso que las muestras de datos y etiquetas (o valores) sean randomizados. Luega en cada iteración se corregirá los valores del vector $\\mathbf{w}$, mediante la siguiente expresión:\n",
    "\n",
    "$\\mathbf{w}(t+1)=\\mathbf{w}(t)+\\alpha y(t)\\mathbf{x}(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## El Perceptron  y la compuerta AND\n",
    "La compuerta **and** consta de 4 ejemplos. Cada uno con 2 características $d=2$. \n",
    "\n",
    "\n",
    "| muestra | x1 | x2 | y  |\n",
    "|---------|----|----|----|\n",
    "| 1       | -1 | -1 | -1 |\n",
    "| 2       | -1 |  1 | -1 |\n",
    "| 3       |  1 | -1 | -1 |\n",
    "| 4       |  1 | 1  |  1 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X es:\n",
      "[[-1 -1]\n",
      " [-1  1]\n",
      " [ 1 -1]\n",
      " [ 1  1]]\n",
      "Y es:\n",
      "[-1 -1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "Y = np.array([-1, -1, -1, 1])\n",
    "print('X es:\\n{}'.format(X))\n",
    "print('Y es:\\n{}'.format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 4, Avg. loss: 0.750000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 8, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 12, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 16, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 20, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "Rendimiento del entrenamiento: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "percept_and = Perceptron(verbose=1, shuffle=True)\n",
    "percept_and.fit(X, Y)\n",
    "print('Rendimiento del entrenamiento: {}'.format(percept_and.score(X,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = percept_and.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio:\n",
    "1. Verifique que la Compuerta **XOR** No tiene solución con un Perceptron\n",
    "2. Verifique que la Computerta **OR** Tiene solución con un Perceptrón\n",
    "3. Escriba un programa en Python que ejecute el algoritmo de aprendizaje del Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes Neuronales Multicapa con Alimentación hacia adelante (Feed Forward)\n",
    "Al añadir capas ocultas a una estructra de redes neuronales se amplía el espacio de hipótesis. Una red sencilla con una capa oculta con 3 perceptrones es la indicada en la siguiente figura. Observe la estructura de izquiera a derecha formada por:\n",
    "\n",
    "* Capa de entrada\n",
    "* Capa oculta\n",
    "* Capa de Salida\n",
    "\n",
    "Note que la capa de salida puede tener más nodos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"252pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 252.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-257 248,-257 248,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"8,-8 8,-245 60,-245 60,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_1</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"80,-35 80,-218 164,-218 164,-35 80,-35\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-202.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">hidden layer</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"184,-89 184,-164 236,-164 236,-89 184,-89\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output</text>\n",
       "</g>\n",
       "<!-- x[0] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-196\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-192.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[0]</text>\n",
       "</g>\n",
       "<!-- h0 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>h0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122\" cy=\"-61\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-57.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h[0]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M43.9237,-180.7762C59.1082,-157.4817 88.244,-112.7848 106.311,-85.0684\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.4488,-86.6639 111.9776,-76.3753 103.5847,-82.8414 109.4488,-86.6639\"/>\n",
       "</g>\n",
       "<!-- h1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>h1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122\" cy=\"-169\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-165.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h[1]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-190.6657C63.8004,-186.8567 80.7424,-181.6586 94.9198,-177.3087\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.1167,-180.6026 104.6501,-174.3232 94.0634,-173.9105 96.1167,-180.6026\"/>\n",
       "</g>\n",
       "<!-- h2 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>h2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h[2]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-183.6978C61.5794,-170.6144 84.2611,-149.7369 100.8935,-134.4276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.5216,-136.7655 108.5089,-127.4179 98.7809,-131.6152 103.5216,-136.7655\"/>\n",
       "</g>\n",
       "<!-- x[1] -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-142\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-138.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[1]</text>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-129.6978C61.5794,-116.6144 84.2611,-95.7369 100.8935,-80.4276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.5216,-82.7655 108.5089,-73.4179 98.7809,-77.6152 103.5216,-82.7655\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-147.3343C63.8004,-151.1433 80.7424,-156.3414 94.9198,-160.6913\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0634,-164.0895 104.6501,-163.6768 96.1167,-157.3974 94.0634,-164.0895\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-136.6657C63.8004,-132.8567 80.7424,-127.6586 94.9198,-123.3087\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.1167,-126.6026 104.6501,-120.3232 94.0634,-119.9105 96.1167,-126.6026\"/>\n",
       "</g>\n",
       "<!-- x[2] -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-88\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[2]</text>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h0 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-82.6657C63.8004,-78.8567 80.7424,-73.6586 94.9198,-69.3087\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.1167,-72.6026 104.6501,-66.3232 94.0634,-65.9105 96.1167,-72.6026\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-100.3022C61.5794,-113.3856 84.2611,-134.2631 100.8935,-149.5724\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.7809,-152.3848 108.5089,-156.5821 103.5216,-147.2345 98.7809,-152.3848\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h2 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-93.3343C63.8004,-97.1433 80.7424,-102.3414 94.9198,-106.6913\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0634,-110.0895 104.6501,-109.6768 96.1167,-103.3974 94.0634,-110.0895\"/>\n",
       "</g>\n",
       "<!-- x[3] -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x[3]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-34\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[3]</text>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h0 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-39.3343C63.8004,-43.1433 80.7424,-48.3414 94.9198,-52.6913\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0634,-56.0895 104.6501,-55.6768 96.1167,-49.3974 94.0634,-56.0895\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M43.9237,-49.2238C59.1082,-72.5183 88.244,-117.2152 106.311,-144.9316\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.5847,-147.1586 111.9776,-153.6247 109.4488,-143.3361 103.5847,-147.1586\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h2 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-46.3022C61.5794,-59.3856 84.2611,-80.2631 100.8935,-95.5724\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.7809,-98.3848 108.5089,-102.5821 103.5216,-93.2345 98.7809,-98.3848\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"210\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- h0&#45;&gt;y -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>h0&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M137.7326,-70.6541C150.9973,-78.7938 170.2058,-90.5808 185.4857,-99.9571\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.0498,-103.1824 194.4036,-105.4295 187.7109,-97.2161 184.0498,-103.1824\"/>\n",
       "</g>\n",
       "<!-- h1&#45;&gt;y -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>h1&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M137.7326,-159.3459C150.9973,-151.2062 170.2058,-139.4192 185.4857,-130.0429\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"187.7109,-132.7839 194.4036,-124.5705 184.0498,-126.8176 187.7109,-132.7839\"/>\n",
       "</g>\n",
       "<!-- h2&#45;&gt;y -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>h2&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M140.2337,-115C152.1508,-115 167.9616,-115 181.5183,-115\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"181.7897,-118.5001 191.7897,-115 181.7897,-111.5001 181.7897,-118.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f4b27166b70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mglearn.plots.plot_single_hidden_layer_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La complejidad del problema puede requerir incrementar el número de capas ocultas. Para entrenar este tipo de redes se necesita el algoritmo de **back propagation** o retropropagación del error, ya que al tener varias neuronas en una capa oculta se dispone de un vector de hipótesis cuyos valores correctos no se conocen previamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"378pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 378.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-257 374,-257 374,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"8,-8 8,-245 60,-245 60,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_1</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"80,-35 80,-218 175,-218 175,-35 80,-35\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.5\" y=\"-202.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">hidden layer 1</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"195,-35 195,-218 290,-218 290,-35 195,-35\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.5\" y=\"-202.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">hidden layer 2</text>\n",
       "</g>\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_3</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"310,-89 310,-164 362,-164 362,-89 310,-89\"/>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output</text>\n",
       "</g>\n",
       "<!-- x[0] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-196\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-192.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[0]</text>\n",
       "</g>\n",
       "<!-- h1[0] -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>h1[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-61\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-57.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h1[0]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M44.308,-181.0368C60.4387,-157.6212 91.7762,-112.1314 110.9005,-84.3702\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"113.8551,-86.2509 116.6459,-76.0302 108.0905,-82.2797 113.8551,-86.2509\"/>\n",
       "</g>\n",
       "<!-- h1[1] -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>h1[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-169\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-165.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h1[1]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-190.9215C65.1319,-186.9617 84.3149,-181.3924 99.9321,-176.8584\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.9582,-180.2051 109.5858,-174.0557 99.0065,-173.4827 100.9582,-180.2051\"/>\n",
       "</g>\n",
       "<!-- h1[2] -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>h1[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h1[2]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-184.0482C62.9704,-170.7677 87.7855,-149.1546 105.5924,-133.6453\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.9791,-136.208 113.2212,-127.0009 103.3816,-130.9294 107.9791,-136.208\"/>\n",
       "</g>\n",
       "<!-- x[1] -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-142\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-138.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[1]</text>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-130.0482C62.9704,-116.7677 87.7855,-95.1546 105.5924,-79.6453\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.9791,-82.208 113.2212,-73.0009 103.3816,-76.9294 107.9791,-82.208\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-147.0785C65.1319,-151.0383 84.3149,-156.6076 99.9321,-161.1416\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.0065,-164.5173 109.5858,-163.9443 100.9582,-157.7949 99.0065,-164.5173\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-136.9215C65.1319,-132.9617 84.3149,-127.3924 99.9321,-122.8584\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.9582,-126.2051 109.5858,-120.0557 99.0065,-119.4827 100.9582,-126.2051\"/>\n",
       "</g>\n",
       "<!-- x[2] -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-88\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[2]</text>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-82.9215C65.1319,-78.9617 84.3149,-73.3924 99.9321,-68.8584\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.9582,-72.2051 109.5858,-66.0557 99.0065,-65.4827 100.9582,-72.2051\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-99.9518C62.9704,-113.2323 87.7855,-134.8454 105.5924,-150.3547\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3816,-153.0706 113.2212,-156.9991 107.9791,-147.792 103.3816,-153.0706\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-93.0785C65.1319,-97.0383 84.3149,-102.6076 99.9321,-107.1416\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.0065,-110.5173 109.5858,-109.9443 100.9582,-103.7949 99.0065,-110.5173\"/>\n",
       "</g>\n",
       "<!-- x[3] -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x[3]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-34\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[3]</text>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-39.0785C65.1319,-43.0383 84.3149,-48.6076 99.9321,-53.1416\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.0065,-56.5173 109.5858,-55.9443 100.9582,-49.7949 99.0065,-56.5173\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M44.308,-48.9632C60.4387,-72.3788 91.7762,-117.8686 110.9005,-145.6298\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.0905,-147.7203 116.6459,-153.9698 113.8551,-143.7491 108.0905,-147.7203\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-45.9518C62.9704,-59.2323 87.7855,-80.8454 105.5924,-96.3547\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3816,-99.0706 113.2212,-102.9991 107.9791,-93.792 103.3816,-99.0706\"/>\n",
       "</g>\n",
       "<!-- h2[0] -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>h2[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"242\" cy=\"-61\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"242\" y=\"-57.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h2[0]</text>\n",
       "</g>\n",
       "<!-- h1[0]&#45;&gt;h2[0] -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>h1[0]&#45;&gt;h2[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.2221,-61C163.6433,-61 192.3671,-61 213.7431,-61\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"213.8632,-64.5001 223.8632,-61 213.8631,-57.5001 213.8632,-64.5001\"/>\n",
       "</g>\n",
       "<!-- h2[1] -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>h2[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"242\" cy=\"-169\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"242\" y=\"-165.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h2[1]</text>\n",
       "</g>\n",
       "<!-- h1[0]&#45;&gt;h2[1] -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>h1[0]&#45;&gt;h2[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M140.192,-73.389C160.0161,-92.0064 197.669,-127.3674 221.211,-149.4764\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"218.9222,-152.1284 228.6077,-156.4228 223.7142,-147.0258 218.9222,-152.1284\"/>\n",
       "</g>\n",
       "<!-- h2[2] -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>h2[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"242\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"242\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h2[2]</text>\n",
       "</g>\n",
       "<!-- h1[0]&#45;&gt;h2[2] -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>h1[0]&#45;&gt;h2[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-68.7369C162.5201,-77.679 194.0899,-92.5031 216.3526,-102.9569\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.0145,-106.1951 225.5539,-107.2775 217.9898,-99.8589 215.0145,-106.1951\"/>\n",
       "</g>\n",
       "<!-- h1[1]&#45;&gt;h2[0] -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>h1[1]&#45;&gt;h2[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M140.192,-156.611C160.0161,-137.9936 197.669,-102.6326 221.211,-80.5236\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"223.7142,-82.9742 228.6077,-73.5772 218.9222,-77.8716 223.7142,-82.9742\"/>\n",
       "</g>\n",
       "<!-- h1[1]&#45;&gt;h2[1] -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>h1[1]&#45;&gt;h2[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.2221,-169C163.6433,-169 192.3671,-169 213.7431,-169\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"213.8632,-172.5001 223.8632,-169 213.8631,-165.5001 213.8632,-172.5001\"/>\n",
       "</g>\n",
       "<!-- h1[1]&#45;&gt;h2[2] -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>h1[1]&#45;&gt;h2[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-161.2631C162.5201,-152.321 194.0899,-137.4969 216.3526,-127.0431\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.9898,-130.1411 225.5539,-122.7225 215.0145,-123.8049 217.9898,-130.1411\"/>\n",
       "</g>\n",
       "<!-- h1[2]&#45;&gt;h2[0] -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>h1[2]&#45;&gt;h2[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-107.2631C162.5201,-98.321 194.0899,-83.4969 216.3526,-73.0431\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.9898,-76.1411 225.5539,-68.7225 215.0145,-69.8049 217.9898,-76.1411\"/>\n",
       "</g>\n",
       "<!-- h1[2]&#45;&gt;h2[1] -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>h1[2]&#45;&gt;h2[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-122.7369C162.5201,-131.679 194.0899,-146.5031 216.3526,-156.9569\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.0145,-160.1951 225.5539,-161.2775 217.9898,-153.8589 215.0145,-160.1951\"/>\n",
       "</g>\n",
       "<!-- h1[2]&#45;&gt;h2[2] -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>h1[2]&#45;&gt;h2[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.2221,-115C163.6433,-115 192.3671,-115 213.7431,-115\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"213.8632,-118.5001 223.8632,-115 213.8631,-111.5001 213.8632,-118.5001\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"336\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- h2[0]&#45;&gt;y -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>h2[0]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M257.9458,-70.1604C272.6188,-78.5895 294.6007,-91.2174 311.5017,-100.9265\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"309.8752,-104.0285 320.2897,-105.9749 313.3621,-97.9588 309.8752,-104.0285\"/>\n",
       "</g>\n",
       "<!-- h2[1]&#45;&gt;y -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>h2[1]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M257.9458,-159.8396C272.6188,-151.4105 294.6007,-138.7826 311.5017,-129.0735\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"313.3621,-132.0412 320.2897,-124.0251 309.8752,-125.9715 313.3621,-132.0412\"/>\n",
       "</g>\n",
       "<!-- h2[2]&#45;&gt;y -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>h2[2]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M260.1241,-115C273.6484,-115 292.3808,-115 307.8486,-115\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"307.9315,-118.5001 317.9315,-115 307.9315,-111.5001 307.9315,-118.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f4b2711d240>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mglearn.plots.plot_two_hidden_layer_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio:\n",
    "Utilice una red neuronal multicapa para clasificar los datos del dataset moons, con:\n",
    "* Algoritmo de descenso de gradiente estocástico,\n",
    "* 50 neuronas en la capa oculta,\n",
    "* Función de activación relu, luego con sigmoid\n",
    "* Pruebe con algoritmo lbfgs y sgd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
      " 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "# print(X[0:5,:])\n",
    "print(y)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Realicemos un plot de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y, cmap=cm_bright)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67948739\n",
      "Iteration 2, loss = 0.67915809\n",
      "Iteration 3, loss = 0.67868928\n",
      "Iteration 4, loss = 0.67809569\n",
      "Iteration 5, loss = 0.67739037\n",
      "Iteration 6, loss = 0.67658609\n",
      "Iteration 7, loss = 0.67569401\n",
      "Iteration 8, loss = 0.67472395\n",
      "Iteration 9, loss = 0.67368554\n",
      "Iteration 10, loss = 0.67258706\n",
      "Iteration 11, loss = 0.67143620\n",
      "Iteration 12, loss = 0.67023946\n",
      "Iteration 13, loss = 0.66900273\n",
      "Iteration 14, loss = 0.66773188\n",
      "Iteration 15, loss = 0.66643203\n",
      "Iteration 16, loss = 0.66510874\n",
      "Iteration 17, loss = 0.66376526\n",
      "Iteration 18, loss = 0.66240714\n",
      "Iteration 19, loss = 0.66103601\n",
      "Iteration 20, loss = 0.65965500\n",
      "Iteration 21, loss = 0.65826646\n",
      "Iteration 22, loss = 0.65687314\n",
      "Iteration 23, loss = 0.65548093\n",
      "Iteration 24, loss = 0.65408727\n",
      "Iteration 25, loss = 0.65269483\n",
      "Iteration 26, loss = 0.65130382\n",
      "Iteration 27, loss = 0.64991403\n",
      "Iteration 28, loss = 0.64852883\n",
      "Iteration 29, loss = 0.64714910\n",
      "Iteration 30, loss = 0.64577370\n",
      "Iteration 31, loss = 0.64440392\n",
      "Iteration 32, loss = 0.64304056\n",
      "Iteration 33, loss = 0.64168409\n",
      "Iteration 34, loss = 0.64033565\n",
      "Iteration 35, loss = 0.63899685\n",
      "Iteration 36, loss = 0.63766786\n",
      "Iteration 37, loss = 0.63634986\n",
      "Iteration 38, loss = 0.63504046\n",
      "Iteration 39, loss = 0.63374092\n",
      "Iteration 40, loss = 0.63245082\n",
      "Iteration 41, loss = 0.63117028\n",
      "Iteration 42, loss = 0.62989938\n",
      "Iteration 43, loss = 0.62863815\n",
      "Iteration 44, loss = 0.62738664\n",
      "Iteration 45, loss = 0.62614479\n",
      "Iteration 46, loss = 0.62491189\n",
      "Iteration 47, loss = 0.62368774\n",
      "Iteration 48, loss = 0.62247331\n",
      "Iteration 49, loss = 0.62126771\n",
      "Iteration 50, loss = 0.62007155\n",
      "Iteration 51, loss = 0.61888572\n",
      "Iteration 52, loss = 0.61771018\n",
      "Iteration 53, loss = 0.61654400\n",
      "Iteration 54, loss = 0.61538845\n",
      "Iteration 55, loss = 0.61424232\n",
      "Iteration 56, loss = 0.61310339\n",
      "Iteration 57, loss = 0.61197311\n",
      "Iteration 58, loss = 0.61085129\n",
      "Iteration 59, loss = 0.60973756\n",
      "Iteration 60, loss = 0.60863332\n",
      "Iteration 61, loss = 0.60753848\n",
      "Iteration 62, loss = 0.60645568\n",
      "Iteration 63, loss = 0.60538199\n",
      "Iteration 64, loss = 0.60431628\n",
      "Iteration 65, loss = 0.60325857\n",
      "Iteration 66, loss = 0.60220887\n",
      "Iteration 67, loss = 0.60116726\n",
      "Iteration 68, loss = 0.60013325\n",
      "Iteration 69, loss = 0.59910661\n",
      "Iteration 70, loss = 0.59808737\n",
      "Iteration 71, loss = 0.59707484\n",
      "Iteration 72, loss = 0.59606986\n",
      "Iteration 73, loss = 0.59507221\n",
      "Iteration 74, loss = 0.59408181\n",
      "Iteration 75, loss = 0.59309812\n",
      "Iteration 76, loss = 0.59212093\n",
      "Iteration 77, loss = 0.59115095\n",
      "Iteration 78, loss = 0.59018997\n",
      "Iteration 79, loss = 0.58923588\n",
      "Iteration 80, loss = 0.58828744\n",
      "Iteration 81, loss = 0.58734535\n",
      "Iteration 82, loss = 0.58640980\n",
      "Iteration 83, loss = 0.58548100\n",
      "Iteration 84, loss = 0.58455817\n",
      "Iteration 85, loss = 0.58364056\n",
      "Iteration 86, loss = 0.58273002\n",
      "Iteration 87, loss = 0.58182562\n",
      "Iteration 88, loss = 0.58092783\n",
      "Iteration 89, loss = 0.58003610\n",
      "Iteration 90, loss = 0.57915074\n",
      "Iteration 91, loss = 0.57827187\n",
      "Iteration 92, loss = 0.57739884\n",
      "Iteration 93, loss = 0.57653159\n",
      "Iteration 94, loss = 0.57567015\n",
      "Iteration 95, loss = 0.57481401\n",
      "Iteration 96, loss = 0.57396338\n",
      "Iteration 97, loss = 0.57311730\n",
      "Iteration 98, loss = 0.57227617\n",
      "Iteration 99, loss = 0.57143938\n",
      "Iteration 100, loss = 0.57060728\n",
      "Iteration 101, loss = 0.56978023\n",
      "Iteration 102, loss = 0.56895875\n",
      "Iteration 103, loss = 0.56814203\n",
      "Iteration 104, loss = 0.56732996\n",
      "Iteration 105, loss = 0.56652201\n",
      "Iteration 106, loss = 0.56571803\n",
      "Iteration 107, loss = 0.56491839\n",
      "Iteration 108, loss = 0.56412349\n",
      "Iteration 109, loss = 0.56333252\n",
      "Iteration 110, loss = 0.56254547\n",
      "Iteration 111, loss = 0.56176196\n",
      "Iteration 112, loss = 0.56098270\n",
      "Iteration 113, loss = 0.56020806\n",
      "Iteration 114, loss = 0.55943775\n",
      "Iteration 115, loss = 0.55867174\n",
      "Iteration 116, loss = 0.55791036\n",
      "Iteration 117, loss = 0.55715305\n",
      "Iteration 118, loss = 0.55639927\n",
      "Iteration 119, loss = 0.55564879\n",
      "Iteration 120, loss = 0.55490268\n",
      "Iteration 121, loss = 0.55416094\n",
      "Iteration 122, loss = 0.55342304\n",
      "Iteration 123, loss = 0.55268978\n",
      "Iteration 124, loss = 0.55196093\n",
      "Iteration 125, loss = 0.55123603\n",
      "Iteration 126, loss = 0.55051513\n",
      "Iteration 127, loss = 0.54979818\n",
      "Iteration 128, loss = 0.54908516\n",
      "Iteration 129, loss = 0.54837510\n",
      "Iteration 130, loss = 0.54766853\n",
      "Iteration 131, loss = 0.54696538\n",
      "Iteration 132, loss = 0.54626561\n",
      "Iteration 133, loss = 0.54556928\n",
      "Iteration 134, loss = 0.54487656\n",
      "Iteration 135, loss = 0.54418738\n",
      "Iteration 136, loss = 0.54350124\n",
      "Iteration 137, loss = 0.54281880\n",
      "Iteration 138, loss = 0.54214022\n",
      "Iteration 139, loss = 0.54146561\n",
      "Iteration 140, loss = 0.54079514\n",
      "Iteration 141, loss = 0.54012856\n",
      "Iteration 142, loss = 0.53946562\n",
      "Iteration 143, loss = 0.53880617\n",
      "Iteration 144, loss = 0.53815011\n",
      "Iteration 145, loss = 0.53749743\n",
      "Iteration 146, loss = 0.53684822\n",
      "Iteration 147, loss = 0.53620310\n",
      "Iteration 148, loss = 0.53556167\n",
      "Iteration 149, loss = 0.53492410\n",
      "Iteration 150, loss = 0.53429046\n",
      "Iteration 151, loss = 0.53365973\n",
      "Iteration 152, loss = 0.53303211\n",
      "Iteration 153, loss = 0.53240757\n",
      "Iteration 154, loss = 0.53178585\n",
      "Iteration 155, loss = 0.53116653\n",
      "Iteration 156, loss = 0.53055019\n",
      "Iteration 157, loss = 0.52993690\n",
      "Iteration 158, loss = 0.52932672\n",
      "Iteration 159, loss = 0.52871987\n",
      "Iteration 160, loss = 0.52811606\n",
      "Iteration 161, loss = 0.52751519\n",
      "Iteration 162, loss = 0.52691782\n",
      "Iteration 163, loss = 0.52632434\n",
      "Iteration 164, loss = 0.52573337\n",
      "Iteration 165, loss = 0.52514639\n",
      "Iteration 166, loss = 0.52456305\n",
      "Iteration 167, loss = 0.52398200\n",
      "Iteration 168, loss = 0.52340298\n",
      "Iteration 169, loss = 0.52282711\n",
      "Iteration 170, loss = 0.52225508\n",
      "Iteration 171, loss = 0.52168600\n",
      "Iteration 172, loss = 0.52111966\n",
      "Iteration 173, loss = 0.52055644\n",
      "Iteration 174, loss = 0.51999601\n",
      "Iteration 175, loss = 0.51943866\n",
      "Iteration 176, loss = 0.51888397\n",
      "Iteration 177, loss = 0.51833193\n",
      "Iteration 178, loss = 0.51778249\n",
      "Iteration 179, loss = 0.51723564\n",
      "Iteration 180, loss = 0.51669135\n",
      "Iteration 181, loss = 0.51615009\n",
      "Iteration 182, loss = 0.51561203\n",
      "Iteration 183, loss = 0.51507645\n",
      "Iteration 184, loss = 0.51454417\n",
      "Iteration 185, loss = 0.51401440\n",
      "Iteration 186, loss = 0.51348713\n",
      "Iteration 187, loss = 0.51296232\n",
      "Iteration 188, loss = 0.51244007\n",
      "Iteration 189, loss = 0.51192029\n",
      "Iteration 190, loss = 0.51140290\n",
      "Iteration 191, loss = 0.51088798\n",
      "Iteration 192, loss = 0.51037559\n",
      "Iteration 193, loss = 0.50986578\n",
      "Iteration 194, loss = 0.50935834\n",
      "Iteration 195, loss = 0.50885345\n",
      "Iteration 196, loss = 0.50835003\n",
      "Iteration 197, loss = 0.50784895\n",
      "Iteration 198, loss = 0.50734997\n",
      "Iteration 199, loss = 0.50685322\n",
      "Iteration 200, loss = 0.50635846\n",
      "Iteration 201, loss = 0.50586568\n",
      "Iteration 202, loss = 0.50537497\n",
      "Iteration 203, loss = 0.50488630\n",
      "Iteration 204, loss = 0.50439981\n",
      "Iteration 205, loss = 0.50391539\n",
      "Iteration 206, loss = 0.50343300\n",
      "Iteration 207, loss = 0.50295250\n",
      "Iteration 208, loss = 0.50247438\n",
      "Iteration 209, loss = 0.50199837\n",
      "Iteration 210, loss = 0.50152423\n",
      "Iteration 211, loss = 0.50105137\n",
      "Iteration 212, loss = 0.50058033\n",
      "Iteration 213, loss = 0.50011117\n",
      "Iteration 214, loss = 0.49964392\n",
      "Iteration 215, loss = 0.49917861\n",
      "Iteration 216, loss = 0.49871487\n",
      "Iteration 217, loss = 0.49825310\n",
      "Iteration 218, loss = 0.49779334\n",
      "Iteration 219, loss = 0.49733559\n",
      "Iteration 220, loss = 0.49688009\n",
      "Iteration 221, loss = 0.49642652\n",
      "Iteration 222, loss = 0.49597481\n",
      "Iteration 223, loss = 0.49552494\n",
      "Iteration 224, loss = 0.49507723\n",
      "Iteration 225, loss = 0.49463174\n",
      "Iteration 226, loss = 0.49418847\n",
      "Iteration 227, loss = 0.49374708\n",
      "Iteration 228, loss = 0.49330771\n",
      "Iteration 229, loss = 0.49287017\n",
      "Iteration 230, loss = 0.49243444\n",
      "Iteration 231, loss = 0.49200049\n",
      "Iteration 232, loss = 0.49156847\n",
      "Iteration 233, loss = 0.49113822\n",
      "Iteration 234, loss = 0.49070925\n",
      "Iteration 235, loss = 0.49028184\n",
      "Iteration 236, loss = 0.48985610\n",
      "Iteration 237, loss = 0.48943239\n",
      "Iteration 238, loss = 0.48901071\n",
      "Iteration 239, loss = 0.48859072\n",
      "Iteration 240, loss = 0.48817253\n",
      "Iteration 241, loss = 0.48775661\n",
      "Iteration 242, loss = 0.48734245\n",
      "Iteration 243, loss = 0.48693016\n",
      "Iteration 244, loss = 0.48651966\n",
      "Iteration 245, loss = 0.48611105\n",
      "Iteration 246, loss = 0.48570401\n",
      "Iteration 247, loss = 0.48529852\n",
      "Iteration 248, loss = 0.48489465\n",
      "Iteration 249, loss = 0.48449275\n",
      "Iteration 250, loss = 0.48409258\n",
      "Iteration 251, loss = 0.48369403\n",
      "Iteration 252, loss = 0.48329707\n",
      "Iteration 253, loss = 0.48290181\n",
      "Iteration 254, loss = 0.48250819\n",
      "Iteration 255, loss = 0.48211618\n",
      "Iteration 256, loss = 0.48172580\n",
      "Iteration 257, loss = 0.48133740\n",
      "Iteration 258, loss = 0.48095090\n",
      "Iteration 259, loss = 0.48056597\n",
      "Iteration 260, loss = 0.48018257\n",
      "Iteration 261, loss = 0.47980071\n",
      "Iteration 262, loss = 0.47942038\n",
      "Iteration 263, loss = 0.47904159\n",
      "Iteration 264, loss = 0.47866429\n",
      "Iteration 265, loss = 0.47828848\n",
      "Iteration 266, loss = 0.47791413\n",
      "Iteration 267, loss = 0.47754124\n",
      "Iteration 268, loss = 0.47716994\n",
      "Iteration 269, loss = 0.47680011\n",
      "Iteration 270, loss = 0.47643171\n",
      "Iteration 271, loss = 0.47606487\n",
      "Iteration 272, loss = 0.47569950\n",
      "Iteration 273, loss = 0.47533559\n",
      "Iteration 274, loss = 0.47497323\n",
      "Iteration 275, loss = 0.47461209\n",
      "Iteration 276, loss = 0.47425227\n",
      "Iteration 277, loss = 0.47389381\n",
      "Iteration 278, loss = 0.47353670\n",
      "Iteration 279, loss = 0.47318108\n",
      "Iteration 280, loss = 0.47282687\n",
      "Iteration 281, loss = 0.47247399\n",
      "Iteration 282, loss = 0.47212245\n",
      "Iteration 283, loss = 0.47177237\n",
      "Iteration 284, loss = 0.47142364\n",
      "Iteration 285, loss = 0.47107623\n",
      "Iteration 286, loss = 0.47073031\n",
      "Iteration 287, loss = 0.47038570\n",
      "Iteration 288, loss = 0.47004243\n",
      "Iteration 289, loss = 0.46970070\n",
      "Iteration 290, loss = 0.46936025\n",
      "Iteration 291, loss = 0.46902103\n",
      "Iteration 292, loss = 0.46868301\n",
      "Iteration 293, loss = 0.46834633\n",
      "Iteration 294, loss = 0.46801094\n",
      "Iteration 295, loss = 0.46767678\n",
      "Iteration 296, loss = 0.46734386\n",
      "Iteration 297, loss = 0.46701215\n",
      "Iteration 298, loss = 0.46668166\n",
      "Iteration 299, loss = 0.46635233\n",
      "Iteration 300, loss = 0.46602385\n",
      "Iteration 301, loss = 0.46569654\n",
      "Iteration 302, loss = 0.46537040\n",
      "Iteration 303, loss = 0.46504550\n",
      "Iteration 304, loss = 0.46472182\n",
      "Iteration 305, loss = 0.46439939\n",
      "Iteration 306, loss = 0.46407811\n",
      "Iteration 307, loss = 0.46375800\n",
      "Iteration 308, loss = 0.46343904\n",
      "Iteration 309, loss = 0.46312123\n",
      "Iteration 310, loss = 0.46280456\n",
      "Iteration 311, loss = 0.46248903\n",
      "Iteration 312, loss = 0.46217468\n",
      "Iteration 313, loss = 0.46186155\n",
      "Iteration 314, loss = 0.46154955\n",
      "Iteration 315, loss = 0.46123874\n",
      "Iteration 316, loss = 0.46092915\n",
      "Iteration 317, loss = 0.46062074\n",
      "Iteration 318, loss = 0.46031385\n",
      "Iteration 319, loss = 0.46000837\n",
      "Iteration 320, loss = 0.45970403\n",
      "Iteration 321, loss = 0.45940089\n",
      "Iteration 322, loss = 0.45909896\n",
      "Iteration 323, loss = 0.45879818\n",
      "Iteration 324, loss = 0.45849851\n",
      "Iteration 325, loss = 0.45819995\n",
      "Iteration 326, loss = 0.45790248\n",
      "Iteration 327, loss = 0.45760611\n",
      "Iteration 328, loss = 0.45731082\n",
      "Iteration 329, loss = 0.45701660\n",
      "Iteration 330, loss = 0.45672345\n",
      "Iteration 331, loss = 0.45643136\n",
      "Iteration 332, loss = 0.45614036\n",
      "Iteration 333, loss = 0.45585051\n",
      "Iteration 334, loss = 0.45556149\n",
      "Iteration 335, loss = 0.45527331\n",
      "Iteration 336, loss = 0.45498612\n",
      "Iteration 337, loss = 0.45469994\n",
      "Iteration 338, loss = 0.45441474\n",
      "Iteration 339, loss = 0.45413055\n",
      "Iteration 340, loss = 0.45384733\n",
      "Iteration 341, loss = 0.45356511\n",
      "Iteration 342, loss = 0.45328394\n",
      "Iteration 343, loss = 0.45300394\n",
      "Iteration 344, loss = 0.45272492\n",
      "Iteration 345, loss = 0.45244688\n",
      "Iteration 346, loss = 0.45216982\n",
      "Iteration 347, loss = 0.45189343\n",
      "Iteration 348, loss = 0.45161791\n",
      "Iteration 349, loss = 0.45134332\n",
      "Iteration 350, loss = 0.45106967\n",
      "Iteration 351, loss = 0.45079694\n",
      "Iteration 352, loss = 0.45052494\n",
      "Iteration 353, loss = 0.45025374\n",
      "Iteration 354, loss = 0.44998342\n",
      "Iteration 355, loss = 0.44971405\n",
      "Iteration 356, loss = 0.44944561\n",
      "Iteration 357, loss = 0.44917807\n",
      "Iteration 358, loss = 0.44891143\n",
      "Iteration 359, loss = 0.44864557\n",
      "Iteration 360, loss = 0.44838055\n",
      "Iteration 361, loss = 0.44811647\n",
      "Iteration 362, loss = 0.44785329\n",
      "Iteration 363, loss = 0.44759098\n",
      "Iteration 364, loss = 0.44732965\n",
      "Iteration 365, loss = 0.44706931\n",
      "Iteration 366, loss = 0.44680997\n",
      "Iteration 367, loss = 0.44655160\n",
      "Iteration 368, loss = 0.44629410\n",
      "Iteration 369, loss = 0.44603745\n",
      "Iteration 370, loss = 0.44578175\n",
      "Iteration 371, loss = 0.44552704\n",
      "Iteration 372, loss = 0.44527321\n",
      "Iteration 373, loss = 0.44502030\n",
      "Iteration 374, loss = 0.44476855\n",
      "Iteration 375, loss = 0.44451767\n",
      "Iteration 376, loss = 0.44426775\n",
      "Iteration 377, loss = 0.44401866\n",
      "Iteration 378, loss = 0.44377026\n",
      "Iteration 379, loss = 0.44352269\n",
      "Iteration 380, loss = 0.44327597\n",
      "Iteration 381, loss = 0.44303007\n",
      "Iteration 382, loss = 0.44278501\n",
      "Iteration 383, loss = 0.44254077\n",
      "Iteration 384, loss = 0.44229735\n",
      "Iteration 385, loss = 0.44205475\n",
      "Iteration 386, loss = 0.44181297\n",
      "Iteration 387, loss = 0.44157196\n",
      "Iteration 388, loss = 0.44133173\n",
      "Iteration 389, loss = 0.44109229\n",
      "Iteration 390, loss = 0.44085365\n",
      "Iteration 391, loss = 0.44061580\n",
      "Iteration 392, loss = 0.44037874\n",
      "Iteration 393, loss = 0.44014246\n",
      "Iteration 394, loss = 0.43990697\n",
      "Iteration 395, loss = 0.43967226\n",
      "Iteration 396, loss = 0.43943822\n",
      "Iteration 397, loss = 0.43920498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 398, loss = 0.43897324\n",
      "Iteration 399, loss = 0.43874337\n",
      "Iteration 400, loss = 0.43851436\n",
      "Iteration 401, loss = 0.43828613\n",
      "Iteration 402, loss = 0.43805854\n",
      "Iteration 403, loss = 0.43783175\n",
      "Iteration 404, loss = 0.43760575\n",
      "Iteration 405, loss = 0.43738054\n",
      "Iteration 406, loss = 0.43715614\n",
      "Iteration 407, loss = 0.43693252\n",
      "Iteration 408, loss = 0.43670985\n",
      "Iteration 409, loss = 0.43648799\n",
      "Iteration 410, loss = 0.43626689\n",
      "Iteration 411, loss = 0.43604655\n",
      "Iteration 412, loss = 0.43582697\n",
      "Iteration 413, loss = 0.43560813\n",
      "Iteration 414, loss = 0.43539004\n",
      "Iteration 415, loss = 0.43517269\n",
      "Iteration 416, loss = 0.43495606\n",
      "Iteration 417, loss = 0.43473998\n",
      "Iteration 418, loss = 0.43452459\n",
      "Iteration 419, loss = 0.43430988\n",
      "Iteration 420, loss = 0.43409563\n",
      "Iteration 421, loss = 0.43388189\n",
      "Iteration 422, loss = 0.43366866\n",
      "Iteration 423, loss = 0.43345609\n",
      "Iteration 424, loss = 0.43324418\n",
      "Iteration 425, loss = 0.43303298\n",
      "Iteration 426, loss = 0.43282249\n",
      "Iteration 427, loss = 0.43261266\n",
      "Iteration 428, loss = 0.43240349\n",
      "Iteration 429, loss = 0.43219498\n",
      "Iteration 430, loss = 0.43198713\n",
      "Iteration 431, loss = 0.43177998\n",
      "Iteration 432, loss = 0.43157338\n",
      "Iteration 433, loss = 0.43136742\n",
      "Iteration 434, loss = 0.43116212\n",
      "Iteration 435, loss = 0.43095743\n",
      "Iteration 436, loss = 0.43075313\n",
      "Iteration 437, loss = 0.43054944\n",
      "Iteration 438, loss = 0.43034638\n",
      "Iteration 439, loss = 0.43014394\n",
      "Iteration 440, loss = 0.42994211\n",
      "Iteration 441, loss = 0.42974091\n",
      "Iteration 442, loss = 0.42954041\n",
      "Iteration 443, loss = 0.42934069\n",
      "Iteration 444, loss = 0.42914176\n",
      "Iteration 445, loss = 0.42894347\n",
      "Iteration 446, loss = 0.42874581\n",
      "Iteration 447, loss = 0.42854878\n",
      "Iteration 448, loss = 0.42835237\n",
      "Iteration 449, loss = 0.42815666\n",
      "Iteration 450, loss = 0.42796171\n",
      "Iteration 451, loss = 0.42776739\n",
      "Iteration 452, loss = 0.42757396\n",
      "Iteration 453, loss = 0.42738132\n",
      "Iteration 454, loss = 0.42718941\n",
      "Iteration 455, loss = 0.42699807\n",
      "Iteration 456, loss = 0.42680734\n",
      "Iteration 457, loss = 0.42661706\n",
      "Iteration 458, loss = 0.42642728\n",
      "Iteration 459, loss = 0.42623809\n",
      "Iteration 460, loss = 0.42604948\n",
      "Iteration 461, loss = 0.42586145\n",
      "Iteration 462, loss = 0.42567401\n",
      "Iteration 463, loss = 0.42548715\n",
      "Iteration 464, loss = 0.42530078\n",
      "Iteration 465, loss = 0.42511482\n",
      "Iteration 466, loss = 0.42492942\n",
      "Iteration 467, loss = 0.42474457\n",
      "Iteration 468, loss = 0.42456027\n",
      "Iteration 469, loss = 0.42437653\n",
      "Iteration 470, loss = 0.42419334\n",
      "Iteration 471, loss = 0.42401070\n",
      "Iteration 472, loss = 0.42382866\n",
      "Iteration 473, loss = 0.42364722\n",
      "Iteration 474, loss = 0.42346643\n",
      "Iteration 475, loss = 0.42328619\n",
      "Iteration 476, loss = 0.42310651\n",
      "Iteration 477, loss = 0.42292738\n",
      "Iteration 478, loss = 0.42274879\n",
      "Iteration 479, loss = 0.42257076\n",
      "Iteration 480, loss = 0.42239326\n",
      "Iteration 481, loss = 0.42221631\n",
      "Iteration 482, loss = 0.42203974\n",
      "Iteration 483, loss = 0.42186346\n",
      "Iteration 484, loss = 0.42168768\n",
      "Iteration 485, loss = 0.42151240\n",
      "Iteration 486, loss = 0.42133764\n",
      "Iteration 487, loss = 0.42116337\n",
      "Iteration 488, loss = 0.42098962\n",
      "Iteration 489, loss = 0.42081636\n",
      "Iteration 490, loss = 0.42064375\n",
      "Iteration 491, loss = 0.42047168\n",
      "Iteration 492, loss = 0.42030030\n",
      "Iteration 493, loss = 0.42012944\n",
      "Iteration 494, loss = 0.41995914\n",
      "Iteration 495, loss = 0.41978936\n",
      "Iteration 496, loss = 0.41962009\n",
      "Iteration 497, loss = 0.41945133\n",
      "Iteration 498, loss = 0.41928309\n",
      "Iteration 499, loss = 0.41911535\n",
      "Iteration 500, loss = 0.41894811\n",
      "Iteration 501, loss = 0.41878139\n",
      "Iteration 502, loss = 0.41861519\n",
      "Iteration 503, loss = 0.41844950\n",
      "Iteration 504, loss = 0.41828412\n",
      "Iteration 505, loss = 0.41811906\n",
      "Iteration 506, loss = 0.41795437\n",
      "Iteration 507, loss = 0.41779009\n",
      "Iteration 508, loss = 0.41762626\n",
      "Iteration 509, loss = 0.41746284\n",
      "Iteration 510, loss = 0.41730009\n",
      "Iteration 511, loss = 0.41713809\n",
      "Iteration 512, loss = 0.41697662\n",
      "Iteration 513, loss = 0.41681566\n",
      "Iteration 514, loss = 0.41665518\n",
      "Iteration 515, loss = 0.41649517\n",
      "Iteration 516, loss = 0.41633563\n",
      "Iteration 517, loss = 0.41617656\n",
      "Iteration 518, loss = 0.41601796\n",
      "Iteration 519, loss = 0.41585983\n",
      "Iteration 520, loss = 0.41570221\n",
      "Iteration 521, loss = 0.41554514\n",
      "Iteration 522, loss = 0.41538857\n",
      "Iteration 523, loss = 0.41523247\n",
      "Iteration 524, loss = 0.41507682\n",
      "Iteration 525, loss = 0.41492163\n",
      "Iteration 526, loss = 0.41476690\n",
      "Iteration 527, loss = 0.41461261\n",
      "Iteration 528, loss = 0.41445877\n",
      "Iteration 529, loss = 0.41430538\n",
      "Iteration 530, loss = 0.41415243\n",
      "Iteration 531, loss = 0.41399992\n",
      "Iteration 532, loss = 0.41384784\n",
      "Iteration 533, loss = 0.41369620\n",
      "Iteration 534, loss = 0.41354500\n",
      "Iteration 535, loss = 0.41339417\n",
      "Iteration 536, loss = 0.41324367\n",
      "Iteration 537, loss = 0.41309359\n",
      "Iteration 538, loss = 0.41294393\n",
      "Iteration 539, loss = 0.41279468\n",
      "Iteration 540, loss = 0.41264586\n",
      "Iteration 541, loss = 0.41249752\n",
      "Iteration 542, loss = 0.41234960\n",
      "Iteration 543, loss = 0.41220209\n",
      "Iteration 544, loss = 0.41205499\n",
      "Iteration 545, loss = 0.41190829\n",
      "Iteration 546, loss = 0.41176201\n",
      "Iteration 547, loss = 0.41161613\n",
      "Iteration 548, loss = 0.41147066\n",
      "Iteration 549, loss = 0.41132559\n",
      "Iteration 550, loss = 0.41118101\n",
      "Iteration 551, loss = 0.41103698\n",
      "Iteration 552, loss = 0.41089337\n",
      "Iteration 553, loss = 0.41075017\n",
      "Iteration 554, loss = 0.41060737\n",
      "Iteration 555, loss = 0.41046498\n",
      "Iteration 556, loss = 0.41032299\n",
      "Iteration 557, loss = 0.41018140\n",
      "Iteration 558, loss = 0.41004057\n",
      "Iteration 559, loss = 0.40990027\n",
      "Iteration 560, loss = 0.40976040\n",
      "Iteration 561, loss = 0.40962094\n",
      "Iteration 562, loss = 0.40948189\n",
      "Iteration 563, loss = 0.40934335\n",
      "Iteration 564, loss = 0.40920520\n",
      "Iteration 565, loss = 0.40906746\n",
      "Iteration 566, loss = 0.40893021\n",
      "Iteration 567, loss = 0.40879338\n",
      "Iteration 568, loss = 0.40865695\n",
      "Iteration 569, loss = 0.40852091\n",
      "Iteration 570, loss = 0.40838526\n",
      "Iteration 571, loss = 0.40825001\n",
      "Iteration 572, loss = 0.40811520\n",
      "Iteration 573, loss = 0.40798091\n",
      "Iteration 574, loss = 0.40784715\n",
      "Iteration 575, loss = 0.40771377\n",
      "Iteration 576, loss = 0.40758078\n",
      "Iteration 577, loss = 0.40744817\n",
      "Iteration 578, loss = 0.40731593\n",
      "Iteration 579, loss = 0.40718408\n",
      "Iteration 580, loss = 0.40705259\n",
      "Iteration 581, loss = 0.40692175\n",
      "Iteration 582, loss = 0.40679187\n",
      "Iteration 583, loss = 0.40666238\n",
      "Iteration 584, loss = 0.40653337\n",
      "Iteration 585, loss = 0.40640493\n",
      "Iteration 586, loss = 0.40627699\n",
      "Iteration 587, loss = 0.40614946\n",
      "Iteration 588, loss = 0.40602233\n",
      "Iteration 589, loss = 0.40589558\n",
      "Iteration 590, loss = 0.40576921\n",
      "Iteration 591, loss = 0.40564321\n",
      "Iteration 592, loss = 0.40551758\n",
      "Iteration 593, loss = 0.40539229\n",
      "Iteration 594, loss = 0.40526734\n",
      "Iteration 595, loss = 0.40514275\n",
      "Iteration 596, loss = 0.40501857\n",
      "Iteration 597, loss = 0.40489476\n",
      "Iteration 598, loss = 0.40477129\n",
      "Iteration 599, loss = 0.40464816\n",
      "Iteration 600, loss = 0.40452537\n",
      "Iteration 601, loss = 0.40440293\n",
      "Iteration 602, loss = 0.40428085\n",
      "Iteration 603, loss = 0.40415912\n",
      "Iteration 604, loss = 0.40403811\n",
      "Iteration 605, loss = 0.40391778\n",
      "Iteration 606, loss = 0.40379836\n",
      "Iteration 607, loss = 0.40367934\n",
      "Iteration 608, loss = 0.40356077\n",
      "Iteration 609, loss = 0.40344272\n",
      "Iteration 610, loss = 0.40332503\n",
      "Iteration 611, loss = 0.40320771\n",
      "Iteration 612, loss = 0.40309074\n",
      "Iteration 613, loss = 0.40297412\n",
      "Iteration 614, loss = 0.40285784\n",
      "Iteration 615, loss = 0.40274190\n",
      "Iteration 616, loss = 0.40262629\n",
      "Iteration 617, loss = 0.40251101\n",
      "Iteration 618, loss = 0.40239600\n",
      "Iteration 619, loss = 0.40228120\n",
      "Iteration 620, loss = 0.40216677\n",
      "Iteration 621, loss = 0.40205264\n",
      "Iteration 622, loss = 0.40193882\n",
      "Iteration 623, loss = 0.40182531\n",
      "Iteration 624, loss = 0.40171209\n",
      "Iteration 625, loss = 0.40159919\n",
      "Iteration 626, loss = 0.40148669\n",
      "Iteration 627, loss = 0.40137452\n",
      "Iteration 628, loss = 0.40126264\n",
      "Iteration 629, loss = 0.40115106\n",
      "Iteration 630, loss = 0.40103978\n",
      "Iteration 631, loss = 0.40092878\n",
      "Iteration 632, loss = 0.40081808\n",
      "Iteration 633, loss = 0.40070767\n",
      "Iteration 634, loss = 0.40059754\n",
      "Iteration 635, loss = 0.40048769\n",
      "Iteration 636, loss = 0.40037815\n",
      "Iteration 637, loss = 0.40026911\n",
      "Iteration 638, loss = 0.40016038\n",
      "Iteration 639, loss = 0.40005194\n",
      "Iteration 640, loss = 0.39994378\n",
      "Iteration 641, loss = 0.39983592\n",
      "Iteration 642, loss = 0.39972833\n",
      "Iteration 643, loss = 0.39962102\n",
      "Iteration 644, loss = 0.39951399\n",
      "Iteration 645, loss = 0.39940724\n",
      "Iteration 646, loss = 0.39930076\n",
      "Iteration 647, loss = 0.39919456\n",
      "Iteration 648, loss = 0.39908862\n",
      "Iteration 649, loss = 0.39898296\n",
      "Iteration 650, loss = 0.39887764\n",
      "Iteration 651, loss = 0.39877266\n",
      "Iteration 652, loss = 0.39866794\n",
      "Iteration 653, loss = 0.39856352\n",
      "Iteration 654, loss = 0.39845933\n",
      "Iteration 655, loss = 0.39835382\n",
      "Iteration 656, loss = 0.39824786\n",
      "Iteration 657, loss = 0.39814190\n",
      "Iteration 658, loss = 0.39803594\n",
      "Iteration 659, loss = 0.39793001\n",
      "Iteration 660, loss = 0.39782414\n",
      "Iteration 661, loss = 0.39771833\n",
      "Iteration 662, loss = 0.39761261\n",
      "Iteration 663, loss = 0.39750699\n",
      "Iteration 664, loss = 0.39740149\n",
      "Iteration 665, loss = 0.39729612\n",
      "Iteration 666, loss = 0.39719097\n",
      "Iteration 667, loss = 0.39708595\n",
      "Iteration 668, loss = 0.39698107\n",
      "Iteration 669, loss = 0.39687634\n",
      "Iteration 670, loss = 0.39677176\n",
      "Iteration 671, loss = 0.39666737\n",
      "Iteration 672, loss = 0.39656318\n",
      "Iteration 673, loss = 0.39645918\n",
      "Iteration 674, loss = 0.39635537\n",
      "Iteration 675, loss = 0.39625177\n",
      "Iteration 676, loss = 0.39614833\n",
      "Iteration 677, loss = 0.39604508\n",
      "Iteration 678, loss = 0.39594203\n",
      "Iteration 679, loss = 0.39583919\n",
      "Iteration 680, loss = 0.39573656\n",
      "Iteration 681, loss = 0.39563414\n",
      "Iteration 682, loss = 0.39553184\n",
      "Iteration 683, loss = 0.39542966\n",
      "Iteration 684, loss = 0.39532769\n",
      "Iteration 685, loss = 0.39522591\n",
      "Iteration 686, loss = 0.39512434\n",
      "Iteration 687, loss = 0.39502298\n",
      "Iteration 688, loss = 0.39492171\n",
      "Iteration 689, loss = 0.39482050\n",
      "Iteration 690, loss = 0.39471991\n",
      "Iteration 691, loss = 0.39461998\n",
      "Iteration 692, loss = 0.39452033\n",
      "Iteration 693, loss = 0.39442100\n",
      "Iteration 694, loss = 0.39432193\n",
      "Iteration 695, loss = 0.39422311\n",
      "Iteration 696, loss = 0.39412454\n",
      "Iteration 697, loss = 0.39402621\n",
      "Iteration 698, loss = 0.39392817\n",
      "Iteration 699, loss = 0.39383039\n",
      "Iteration 700, loss = 0.39373286\n",
      "Iteration 701, loss = 0.39363556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42) #, stratify=y\n",
    "red = MLPClassifier(hidden_layer_sizes=(50,), \n",
    "                    solver='sgd', \n",
    "#                     learning_rate_init=0.0001, \n",
    "#                     activation='relu', \n",
    "                    random_state=0, \n",
    "                    verbose=True, \n",
    "                    max_iter=1000)\n",
    "history = red.fit(X_train, y_train) \n",
    "\n",
    "print(len(X_train))\n",
    "# lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89715307,  0.94175457],\n",
       "       [ 1.06821751,  0.75846569],\n",
       "       [-0.4993884 ,  0.13192906],\n",
       "       [-0.46333991,  0.86330772],\n",
       "       [-0.7280717 ,  0.3259131 ],\n",
       "       [ 1.72532644,  0.53367598],\n",
       "       [ 2.11889248,  0.60498388],\n",
       "       [ 1.31702684, -0.2525239 ],\n",
       "       [ 1.11634545,  0.01823342],\n",
       "       [ 0.55039452,  1.16554689],\n",
       "       [ 0.36877983, -0.34894509],\n",
       "       [ 0.42598043, -0.3006242 ],\n",
       "       [-0.24608615,  0.378107  ],\n",
       "       [-0.60690411,  0.50000529],\n",
       "       [ 1.15536561, -0.50593577],\n",
       "       [ 1.26285558,  0.12916271],\n",
       "       [-0.59385445,  0.46769065],\n",
       "       [ 1.12856036,  0.33191968],\n",
       "       [-0.04686928, -0.01567029],\n",
       "       [ 0.8729088 ,  0.08643291],\n",
       "       [ 0.01856462,  1.32827802],\n",
       "       [ 1.00549331,  0.38686701],\n",
       "       [-0.16955317,  0.60660877],\n",
       "       [ 1.50917461, -0.06701048],\n",
       "       [-0.38099245,  1.34740194],\n",
       "       [-0.51699811,  0.74457804],\n",
       "       [ 1.31311917, -0.69665985],\n",
       "       [ 0.77145295, -0.69709227],\n",
       "       [ 0.97370054, -0.08631168],\n",
       "       [-0.74872343, -0.06972957]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La precisión sobre el dataset de entrenamiento es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285714285714286"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = red.score(X_train, y_train)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La precisión sobre el dataset de prueba es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = red.score(X_test, y_test)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_2d_separator(red, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train) \n",
    "plt.xlabel(\"Feature 0\") \n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio\n",
    "Resuelva el problema del XOR con 2 neuronas en la capa oculta. Considere utilizar una función de activación apropiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "Y = np.array([[-1], [1], [1],[-1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pruebe el código utilizando:\n",
    "1. SGD\n",
    "2. lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "red_xor = MLPClassifier(hidden_layer_sizes=(4),\n",
    "                        activation='tanh',\n",
    "                        solver='lbfgs',\n",
    "                        max_iter=1500, verbose=1)\n",
    "#lbfgs / lbfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Entrenando la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=4, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_xor.fit(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_xor.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1, -1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = red_xor.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_grafico = [-1, 1, 1, -1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y_grafico, cmap=cm_bright)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mglearn \n",
    "mglearn.plots.plot_2d_separator(red_xor, X, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y_grafico) \n",
    "plt.xlabel(\"Feature 0\") \n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicios adicionales\n",
    "Resuelva con una arquitectura de red neuronal feed forward multicapa el dataset iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Red Neuronal como Regresión\n",
    "Considere una función $y = x^2-2x+3$. Para el intervalo de $[-5, 5]$ Obtenga 1000 puntos. Puede hacer que la red neuronal reproduzca esta función en el Intervalo propuesto?\n",
    "\n",
    "Que pasa si incrementa el dataset a 2000 muestras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(-5,5, num=1000)\n",
    "# x = np.arange(10)\n",
    "y = x*x-2*x+3\n",
    "# y\n",
    "# print(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.       , -4.9949975, -4.989995 , ...,  4.989995 ,  4.9949975,\n",
       "        5.       ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(-1,1)\n",
    "print(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 106.77519863\n",
      "Iteration 2, loss = 101.93573500\n",
      "Iteration 3, loss = 97.44138476\n",
      "Iteration 4, loss = 92.98556410\n",
      "Iteration 5, loss = 88.63445613\n",
      "Iteration 6, loss = 84.44067036\n",
      "Iteration 7, loss = 80.19360377\n",
      "Iteration 8, loss = 76.05414774\n",
      "Iteration 9, loss = 71.73785185\n",
      "Iteration 10, loss = 67.65048655\n",
      "Iteration 11, loss = 63.33436047\n",
      "Iteration 12, loss = 59.05521976\n",
      "Iteration 13, loss = 55.00526684\n",
      "Iteration 14, loss = 50.80099902\n",
      "Iteration 15, loss = 46.71776236\n",
      "Iteration 16, loss = 42.60699043\n",
      "Iteration 17, loss = 38.70842585\n",
      "Iteration 18, loss = 35.06207394\n",
      "Iteration 19, loss = 31.41041555\n",
      "Iteration 20, loss = 27.91934341\n",
      "Iteration 21, loss = 24.65760502\n",
      "Iteration 22, loss = 21.77795013\n",
      "Iteration 23, loss = 18.95282583\n",
      "Iteration 24, loss = 16.48684057\n",
      "Iteration 25, loss = 14.20326601\n",
      "Iteration 26, loss = 12.21606205\n",
      "Iteration 27, loss = 10.42945430\n",
      "Iteration 28, loss = 8.99334634\n",
      "Iteration 29, loss = 7.67258941\n",
      "Iteration 30, loss = 6.62159264\n",
      "Iteration 31, loss = 5.85313930\n",
      "Iteration 32, loss = 5.15397166\n",
      "Iteration 33, loss = 4.66664490\n",
      "Iteration 34, loss = 4.29997882\n",
      "Iteration 35, loss = 4.00973050\n",
      "Iteration 36, loss = 3.83002567\n",
      "Iteration 37, loss = 3.66552248\n",
      "Iteration 38, loss = 3.56846070\n",
      "Iteration 39, loss = 3.49974451\n",
      "Iteration 40, loss = 3.43802931\n",
      "Iteration 41, loss = 3.38851781\n",
      "Iteration 42, loss = 3.34870204\n",
      "Iteration 43, loss = 3.30754873\n",
      "Iteration 44, loss = 3.27006334\n",
      "Iteration 45, loss = 3.23191549\n",
      "Iteration 46, loss = 3.19798389\n",
      "Iteration 47, loss = 3.15881923\n",
      "Iteration 48, loss = 3.12398761\n",
      "Iteration 49, loss = 3.08891494\n",
      "Iteration 50, loss = 3.05518258\n",
      "Iteration 51, loss = 3.02228503\n",
      "Iteration 52, loss = 2.98870248\n",
      "Iteration 53, loss = 2.95640960\n",
      "Iteration 54, loss = 2.92538429\n",
      "Iteration 55, loss = 2.89472486\n",
      "Iteration 56, loss = 2.86393497\n",
      "Iteration 57, loss = 2.83428864\n",
      "Iteration 58, loss = 2.80546723\n",
      "Iteration 59, loss = 2.77613909\n",
      "Iteration 60, loss = 2.74847788\n",
      "Iteration 61, loss = 2.71961907\n",
      "Iteration 62, loss = 2.69219921\n",
      "Iteration 63, loss = 2.66615171\n",
      "Iteration 64, loss = 2.63840364\n",
      "Iteration 65, loss = 2.61257248\n",
      "Iteration 66, loss = 2.58659977\n",
      "Iteration 67, loss = 2.56129771\n",
      "Iteration 68, loss = 2.53576230\n",
      "Iteration 69, loss = 2.51121896\n",
      "Iteration 70, loss = 2.48727437\n",
      "Iteration 71, loss = 2.46306534\n",
      "Iteration 72, loss = 2.43950304\n",
      "Iteration 73, loss = 2.41600901\n",
      "Iteration 74, loss = 2.39327264\n",
      "Iteration 75, loss = 2.37014063\n",
      "Iteration 76, loss = 2.34922498\n",
      "Iteration 77, loss = 2.32568342\n",
      "Iteration 78, loss = 2.30385835\n",
      "Iteration 79, loss = 2.28249736\n",
      "Iteration 80, loss = 2.26147781\n",
      "Iteration 81, loss = 2.24102367\n",
      "Iteration 82, loss = 2.21980116\n",
      "Iteration 83, loss = 2.19965564\n",
      "Iteration 84, loss = 2.17952082\n",
      "Iteration 85, loss = 2.15919980\n",
      "Iteration 86, loss = 2.13981510\n",
      "Iteration 87, loss = 2.12041540\n",
      "Iteration 88, loss = 2.10079590\n",
      "Iteration 89, loss = 2.08201218\n",
      "Iteration 90, loss = 2.06328630\n",
      "Iteration 91, loss = 2.04475736\n",
      "Iteration 92, loss = 2.02650091\n",
      "Iteration 93, loss = 2.00856011\n",
      "Iteration 94, loss = 1.99019789\n",
      "Iteration 95, loss = 1.97285323\n",
      "Iteration 96, loss = 1.95494887\n",
      "Iteration 97, loss = 1.93820901\n",
      "Iteration 98, loss = 1.92128056\n",
      "Iteration 99, loss = 1.90421199\n",
      "Iteration 100, loss = 1.88787145\n",
      "Iteration 101, loss = 1.87116051\n",
      "Iteration 102, loss = 1.85542656\n",
      "Iteration 103, loss = 1.83952116\n",
      "Iteration 104, loss = 1.82367737\n",
      "Iteration 105, loss = 1.80735096\n",
      "Iteration 106, loss = 1.79261334\n",
      "Iteration 107, loss = 1.77730268\n",
      "Iteration 108, loss = 1.76255185\n",
      "Iteration 109, loss = 1.74711564\n",
      "Iteration 110, loss = 1.73256745\n",
      "Iteration 111, loss = 1.71864018\n",
      "Iteration 112, loss = 1.70413589\n",
      "Iteration 113, loss = 1.68993914\n",
      "Iteration 114, loss = 1.67557527\n",
      "Iteration 115, loss = 1.66277033\n",
      "Iteration 116, loss = 1.64886537\n",
      "Iteration 117, loss = 1.63545912\n",
      "Iteration 118, loss = 1.62234353\n",
      "Iteration 119, loss = 1.60926479\n",
      "Iteration 120, loss = 1.59608739\n",
      "Iteration 121, loss = 1.58354466\n",
      "Iteration 122, loss = 1.57298198\n",
      "Iteration 123, loss = 1.55866775\n",
      "Iteration 124, loss = 1.54671003\n",
      "Iteration 125, loss = 1.53416955\n",
      "Iteration 126, loss = 1.52235157\n",
      "Iteration 127, loss = 1.51104395\n",
      "Iteration 128, loss = 1.49898450\n",
      "Iteration 129, loss = 1.48829695\n",
      "Iteration 130, loss = 1.47636714\n",
      "Iteration 131, loss = 1.46583809\n",
      "Iteration 132, loss = 1.45426817\n",
      "Iteration 133, loss = 1.44382783\n",
      "Iteration 134, loss = 1.43278036\n",
      "Iteration 135, loss = 1.42220962\n",
      "Iteration 136, loss = 1.41182477\n",
      "Iteration 137, loss = 1.40086091\n",
      "Iteration 138, loss = 1.39102621\n",
      "Iteration 139, loss = 1.38060445\n",
      "Iteration 140, loss = 1.37035575\n",
      "Iteration 141, loss = 1.36138056\n",
      "Iteration 142, loss = 1.35122820\n",
      "Iteration 143, loss = 1.34151739\n",
      "Iteration 144, loss = 1.33182779\n",
      "Iteration 145, loss = 1.32203512\n",
      "Iteration 146, loss = 1.31274730\n",
      "Iteration 147, loss = 1.30394872\n",
      "Iteration 148, loss = 1.29489467\n",
      "Iteration 149, loss = 1.28567080\n",
      "Iteration 150, loss = 1.27650162\n",
      "Iteration 151, loss = 1.26764913\n",
      "Iteration 152, loss = 1.25892318\n",
      "Iteration 153, loss = 1.24987353\n",
      "Iteration 154, loss = 1.24178550\n",
      "Iteration 155, loss = 1.23317306\n",
      "Iteration 156, loss = 1.22400791\n",
      "Iteration 157, loss = 1.21573697\n",
      "Iteration 158, loss = 1.20717415\n",
      "Iteration 159, loss = 1.19908129\n",
      "Iteration 160, loss = 1.19098658\n",
      "Iteration 161, loss = 1.18239434\n",
      "Iteration 162, loss = 1.17428373\n",
      "Iteration 163, loss = 1.16631981\n",
      "Iteration 164, loss = 1.15806895\n",
      "Iteration 165, loss = 1.15078312\n",
      "Iteration 166, loss = 1.14228533\n",
      "Iteration 167, loss = 1.13440968\n",
      "Iteration 168, loss = 1.12651864\n",
      "Iteration 169, loss = 1.11887089\n",
      "Iteration 170, loss = 1.11104896\n",
      "Iteration 171, loss = 1.10379271\n",
      "Iteration 172, loss = 1.09612812\n",
      "Iteration 173, loss = 1.08835048\n",
      "Iteration 174, loss = 1.08047533\n",
      "Iteration 175, loss = 1.07339337\n",
      "Iteration 176, loss = 1.06578425\n",
      "Iteration 177, loss = 1.05799446\n",
      "Iteration 178, loss = 1.05068532\n",
      "Iteration 179, loss = 1.04310182\n",
      "Iteration 180, loss = 1.03580186\n",
      "Iteration 181, loss = 1.02825682\n",
      "Iteration 182, loss = 1.02209476\n",
      "Iteration 183, loss = 1.01413750\n",
      "Iteration 184, loss = 1.00641533\n",
      "Iteration 185, loss = 0.99900662\n",
      "Iteration 186, loss = 0.99193449\n",
      "Iteration 187, loss = 0.98465721\n",
      "Iteration 188, loss = 0.97763478\n",
      "Iteration 189, loss = 0.97007367\n",
      "Iteration 190, loss = 0.96309404\n",
      "Iteration 191, loss = 0.95582063\n",
      "Iteration 192, loss = 0.94906410\n",
      "Iteration 193, loss = 0.94163411\n",
      "Iteration 194, loss = 0.93453698\n",
      "Iteration 195, loss = 0.92752973\n",
      "Iteration 196, loss = 0.92033705\n",
      "Iteration 197, loss = 0.91341787\n",
      "Iteration 198, loss = 0.90634334\n",
      "Iteration 199, loss = 0.89993809\n",
      "Iteration 200, loss = 0.89254960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "red_funcion = MLPRegressor(hidden_layer_sizes=(500,), solver='adam', activation='relu', verbose=1)\n",
    "red_funcion.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9785230656126481"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_funcion.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(-10,10,500)\n",
    "y_real = x_test*x_test-2*x_test+3\n",
    "x_test = x_test.reshape(-1,1)\n",
    "# x_test\n",
    "y_pred = red_funcion.predict(x_test)\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_test, y_real, color='blue')\n",
    "plt.plot(x_test, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Qué sucede si deseo calcular para un valor fuera del rango como por ejemplo $x=5.5$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212.25"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_num = 15.5\n",
    "y_num = x_num*x_num-2*x_num+3\n",
    "y_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59.94639315])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_num = np.asarray(x_num)\n",
    "x_num = x_num.reshape(-1,1)\n",
    "y_num_pred = red_funcion.predict(x_num)\n",
    "y_num_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resolviendo el problema de clasificación del iris dataset\n",
    "Considere el problema de clasificar las flores iris según la longitud y ancho del sépalo y el pétolo. Resuelva este problema utilizando Redes Neuronales Multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "print(iris_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los primeros 5 datos del dataset son $\\mathbf{\\mathcal{X}}:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset['data'][0:5,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las etiquetas para las **muestras** o **samples** anteriores son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset['target'][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Paso los datos del iris dataset a una variable $\\mathbf{\\mathcal{X}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = iris_dataset['data']\n",
    "Y = iris_dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Codificando las clases como **ONE HOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_one_hot = np_utils.to_categorical(Y)\n",
    "Y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Escalando los valores de $\\mathbf{\\mathcal{X}}$. Sin embargo primero pruebe el entrenamiento sin escalar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La media [5.84333333 3.05733333 3.758      1.19933333]\n",
      "Escalado de X, se muestra primeras 5 filas \n",
      "[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
      " [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
      " [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
      " [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
      " [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]\n",
      "min -2.43394714190809\n",
      "max 3.0907752482994253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "funcion_escalado = scaler.fit(X)\n",
    "# X_escalado = X\n",
    "X_escalado = funcion_escalado.transform(X)\n",
    "print('La media {}'.format(scaler.mean_))\n",
    "print('Escalado de X, se muestra primeras 5 filas \\n{}'.format(X_escalado[0:5,:]))\n",
    "print('min {}'.format(X_escalado.min()))\n",
    "print('max {}'.format(X_escalado.max()))\n",
    "# StandardScaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dividiendo el dataset en train y test. Los datos se deben aleatorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31099753, -0.36217625,  0.53540856,  0.26414192],\n",
       "       [-0.41600969, -1.28296331,  0.13754657,  0.13250973],\n",
       "       [-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
       "       [-1.02184904,  0.78880759, -1.2833891 , -1.3154443 ],\n",
       "       [ 0.31099753, -0.13197948,  0.64908342,  0.79067065]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_escalado, Y_one_hot, test_size=0.2, random_state=73)\n",
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creando un Modelo de Redes Neuronales Artificiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "red_iris = MLPClassifier(hidden_layer_sizes=(3,),\n",
    "                        activation='tanh',\n",
    "                        solver='sgd',\n",
    "                        momentum=0.95,\n",
    "                        max_iter=1500, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ajuste de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.62087364\n",
      "Iteration 2, loss = 2.61809427\n",
      "Iteration 3, loss = 2.61403511\n",
      "Iteration 4, loss = 2.60876721\n",
      "Iteration 5, loss = 2.60235984\n",
      "Iteration 6, loss = 2.59488046\n",
      "Iteration 7, loss = 2.58639464\n",
      "Iteration 8, loss = 2.57696608\n",
      "Iteration 9, loss = 2.56665658\n",
      "Iteration 10, loss = 2.55552605\n",
      "Iteration 11, loss = 2.54363257\n",
      "Iteration 12, loss = 2.53103237\n",
      "Iteration 13, loss = 2.51777991\n",
      "Iteration 14, loss = 2.50392794\n",
      "Iteration 15, loss = 2.48952753\n",
      "Iteration 16, loss = 2.47462821\n",
      "Iteration 17, loss = 2.45927800\n",
      "Iteration 18, loss = 2.44352357\n",
      "Iteration 19, loss = 2.42741030\n",
      "Iteration 20, loss = 2.41098241\n",
      "Iteration 21, loss = 2.39428309\n",
      "Iteration 22, loss = 2.37735456\n",
      "Iteration 23, loss = 2.36023823\n",
      "Iteration 24, loss = 2.34297470\n",
      "Iteration 25, loss = 2.32560389\n",
      "Iteration 26, loss = 2.30816500\n",
      "Iteration 27, loss = 2.29069653\n",
      "Iteration 28, loss = 2.27323622\n",
      "Iteration 29, loss = 2.25582092\n",
      "Iteration 30, loss = 2.23848653\n",
      "Iteration 31, loss = 2.22126775\n",
      "Iteration 32, loss = 2.20419792\n",
      "Iteration 33, loss = 2.18730881\n",
      "Iteration 34, loss = 2.17063034\n",
      "Iteration 35, loss = 2.15419045\n",
      "Iteration 36, loss = 2.13801481\n",
      "Iteration 37, loss = 2.12212669\n",
      "Iteration 38, loss = 2.10654680\n",
      "Iteration 39, loss = 2.09129321\n",
      "Iteration 40, loss = 2.07638127\n",
      "Iteration 41, loss = 2.06182361\n",
      "Iteration 42, loss = 2.04763018\n",
      "Iteration 43, loss = 2.03380831\n",
      "Iteration 44, loss = 2.02036280\n",
      "Iteration 45, loss = 2.00729607\n",
      "Iteration 46, loss = 1.99460833\n",
      "Iteration 47, loss = 1.98229769\n",
      "Iteration 48, loss = 1.97036041\n",
      "Iteration 49, loss = 1.95879106\n",
      "Iteration 50, loss = 1.94758268\n",
      "Iteration 51, loss = 1.93672702\n",
      "Iteration 52, loss = 1.92621471\n",
      "Iteration 53, loss = 1.91603544\n",
      "Iteration 54, loss = 1.90617811\n",
      "Iteration 55, loss = 1.89663104\n",
      "Iteration 56, loss = 1.88738210\n",
      "Iteration 57, loss = 1.87841886\n",
      "Iteration 58, loss = 1.86972868\n",
      "Iteration 59, loss = 1.86129891\n",
      "Iteration 60, loss = 1.85311693\n",
      "Iteration 61, loss = 1.84517024\n",
      "Iteration 62, loss = 1.83744659\n",
      "Iteration 63, loss = 1.82993399\n",
      "Iteration 64, loss = 1.82262083\n",
      "Iteration 65, loss = 1.81549584\n",
      "Iteration 66, loss = 1.80854820\n",
      "Iteration 67, loss = 1.80176753\n",
      "Iteration 68, loss = 1.79514392\n",
      "Iteration 69, loss = 1.78866794\n",
      "Iteration 70, loss = 1.78233062\n",
      "Iteration 71, loss = 1.77612348\n",
      "Iteration 72, loss = 1.77003852\n",
      "Iteration 73, loss = 1.76406820\n",
      "Iteration 74, loss = 1.75820543\n",
      "Iteration 75, loss = 1.75244355\n",
      "Iteration 76, loss = 1.74677634\n",
      "Iteration 77, loss = 1.74119797\n",
      "Iteration 78, loss = 1.73570299\n",
      "Iteration 79, loss = 1.73028634\n",
      "Iteration 80, loss = 1.72494328\n",
      "Iteration 81, loss = 1.71966941\n",
      "Iteration 82, loss = 1.71446064\n",
      "Iteration 83, loss = 1.70931318\n",
      "Iteration 84, loss = 1.70422349\n",
      "Iteration 85, loss = 1.69918830\n",
      "Iteration 86, loss = 1.69420458\n",
      "Iteration 87, loss = 1.68926951\n",
      "Iteration 88, loss = 1.68438049\n",
      "Iteration 89, loss = 1.67953511\n",
      "Iteration 90, loss = 1.67473113\n",
      "Iteration 91, loss = 1.66996649\n",
      "Iteration 92, loss = 1.66523928\n",
      "Iteration 93, loss = 1.66054772\n",
      "Iteration 94, loss = 1.65589019\n",
      "Iteration 95, loss = 1.65126516\n",
      "Iteration 96, loss = 1.64667123\n",
      "Iteration 97, loss = 1.64210711\n",
      "Iteration 98, loss = 1.63757161\n",
      "Iteration 99, loss = 1.63306359\n",
      "Iteration 100, loss = 1.62858206\n",
      "Iteration 101, loss = 1.62412604\n",
      "Iteration 102, loss = 1.61969465\n",
      "Iteration 103, loss = 1.61528709\n",
      "Iteration 104, loss = 1.61090260\n",
      "Iteration 105, loss = 1.60654047\n",
      "Iteration 106, loss = 1.60220006\n",
      "Iteration 107, loss = 1.59788077\n",
      "Iteration 108, loss = 1.59358203\n",
      "Iteration 109, loss = 1.58930334\n",
      "Iteration 110, loss = 1.58504420\n",
      "Iteration 111, loss = 1.58080418\n",
      "Iteration 112, loss = 1.57658285\n",
      "Iteration 113, loss = 1.57237985\n",
      "Iteration 114, loss = 1.56819481\n",
      "Iteration 115, loss = 1.56402741\n",
      "Iteration 116, loss = 1.55987734\n",
      "Iteration 117, loss = 1.55574431\n",
      "Iteration 118, loss = 1.55162806\n",
      "Iteration 119, loss = 1.54752836\n",
      "Iteration 120, loss = 1.54344497\n",
      "Iteration 121, loss = 1.53937769\n",
      "Iteration 122, loss = 1.53532632\n",
      "Iteration 123, loss = 1.53129070\n",
      "Iteration 124, loss = 1.52727065\n",
      "Iteration 125, loss = 1.52326604\n",
      "Iteration 126, loss = 1.51927671\n",
      "Iteration 127, loss = 1.51530254\n",
      "Iteration 128, loss = 1.51134343\n",
      "Iteration 129, loss = 1.50739926\n",
      "Iteration 130, loss = 1.50346995\n",
      "Iteration 131, loss = 1.49955539\n",
      "Iteration 132, loss = 1.49565553\n",
      "Iteration 133, loss = 1.49177029\n",
      "Iteration 134, loss = 1.48789961\n",
      "Iteration 135, loss = 1.48404344\n",
      "Iteration 136, loss = 1.48020173\n",
      "Iteration 137, loss = 1.47637444\n",
      "Iteration 138, loss = 1.47256154\n",
      "Iteration 139, loss = 1.46876300\n",
      "Iteration 140, loss = 1.46497880\n",
      "Iteration 141, loss = 1.46120892\n",
      "Iteration 142, loss = 1.45745335\n",
      "Iteration 143, loss = 1.45371209\n",
      "Iteration 144, loss = 1.44998513\n",
      "Iteration 145, loss = 1.44627248\n",
      "Iteration 146, loss = 1.44257414\n",
      "Iteration 147, loss = 1.43889011\n",
      "Iteration 148, loss = 1.43522043\n",
      "Iteration 149, loss = 1.43156509\n",
      "Iteration 150, loss = 1.42792412\n",
      "Iteration 151, loss = 1.42429755\n",
      "Iteration 152, loss = 1.42068539\n",
      "Iteration 153, loss = 1.41708768\n",
      "Iteration 154, loss = 1.41350444\n",
      "Iteration 155, loss = 1.40993571\n",
      "Iteration 156, loss = 1.40638151\n",
      "Iteration 157, loss = 1.40284190\n",
      "Iteration 158, loss = 1.39931690\n",
      "Iteration 159, loss = 1.39580654\n",
      "Iteration 160, loss = 1.39231088\n",
      "Iteration 161, loss = 1.38882995\n",
      "Iteration 162, loss = 1.38536379\n",
      "Iteration 163, loss = 1.38191244\n",
      "Iteration 164, loss = 1.37847595\n",
      "Iteration 165, loss = 1.37505436\n",
      "Iteration 166, loss = 1.37164772\n",
      "Iteration 167, loss = 1.36825606\n",
      "Iteration 168, loss = 1.36487943\n",
      "Iteration 169, loss = 1.36151787\n",
      "Iteration 170, loss = 1.35817144\n",
      "Iteration 171, loss = 1.35484016\n",
      "Iteration 172, loss = 1.35152408\n",
      "Iteration 173, loss = 1.34822325\n",
      "Iteration 174, loss = 1.34493771\n",
      "Iteration 175, loss = 1.34166749\n",
      "Iteration 176, loss = 1.33841264\n",
      "Iteration 177, loss = 1.33517319\n",
      "Iteration 178, loss = 1.33194919\n",
      "Iteration 179, loss = 1.32874066\n",
      "Iteration 180, loss = 1.32554765\n",
      "Iteration 181, loss = 1.32237019\n",
      "Iteration 182, loss = 1.31920831\n",
      "Iteration 183, loss = 1.31606205\n",
      "Iteration 184, loss = 1.31293142\n",
      "Iteration 185, loss = 1.30981647\n",
      "Iteration 186, loss = 1.30671721\n",
      "Iteration 187, loss = 1.30363367\n",
      "Iteration 188, loss = 1.30056589\n",
      "Iteration 189, loss = 1.29751386\n",
      "Iteration 190, loss = 1.29447762\n",
      "Iteration 191, loss = 1.29145719\n",
      "Iteration 192, loss = 1.28845258\n",
      "Iteration 193, loss = 1.28546380\n",
      "Iteration 194, loss = 1.28249086\n",
      "Iteration 195, loss = 1.27953378\n",
      "Iteration 196, loss = 1.27659256\n",
      "Iteration 197, loss = 1.27366720\n",
      "Iteration 198, loss = 1.27075772\n",
      "Iteration 199, loss = 1.26786411\n",
      "Iteration 200, loss = 1.26498637\n",
      "Iteration 201, loss = 1.26212450\n",
      "Iteration 202, loss = 1.25927850\n",
      "Iteration 203, loss = 1.25644834\n",
      "Iteration 204, loss = 1.25363404\n",
      "Iteration 205, loss = 1.25083557\n",
      "Iteration 206, loss = 1.24805292\n",
      "Iteration 207, loss = 1.24528608\n",
      "Iteration 208, loss = 1.24253502\n",
      "Iteration 209, loss = 1.23979973\n",
      "Iteration 210, loss = 1.23708018\n",
      "Iteration 211, loss = 1.23437635\n",
      "Iteration 212, loss = 1.23168821\n",
      "Iteration 213, loss = 1.22901573\n",
      "Iteration 214, loss = 1.22635888\n",
      "Iteration 215, loss = 1.22371764\n",
      "Iteration 216, loss = 1.22109195\n",
      "Iteration 217, loss = 1.21848179\n",
      "Iteration 218, loss = 1.21588711\n",
      "Iteration 219, loss = 1.21330788\n",
      "Iteration 220, loss = 1.21074405\n",
      "Iteration 221, loss = 1.20819557\n",
      "Iteration 222, loss = 1.20566240\n",
      "Iteration 223, loss = 1.20314448\n",
      "Iteration 224, loss = 1.20064177\n",
      "Iteration 225, loss = 1.19815422\n",
      "Iteration 226, loss = 1.19568176\n",
      "Iteration 227, loss = 1.19322434\n",
      "Iteration 228, loss = 1.19078191\n",
      "Iteration 229, loss = 1.18835439\n",
      "Iteration 230, loss = 1.18594174\n",
      "Iteration 231, loss = 1.18354388\n",
      "Iteration 232, loss = 1.18116075\n",
      "Iteration 233, loss = 1.17879229\n",
      "Iteration 234, loss = 1.17643842\n",
      "Iteration 235, loss = 1.17409908\n",
      "Iteration 236, loss = 1.17177419\n",
      "Iteration 237, loss = 1.16946368\n",
      "Iteration 238, loss = 1.16716748\n",
      "Iteration 239, loss = 1.16488550\n",
      "Iteration 240, loss = 1.16261769\n",
      "Iteration 241, loss = 1.16036394\n",
      "Iteration 242, loss = 1.15812420\n",
      "Iteration 243, loss = 1.15589836\n",
      "Iteration 244, loss = 1.15368637\n",
      "Iteration 245, loss = 1.15148812\n",
      "Iteration 246, loss = 1.14930354\n",
      "Iteration 247, loss = 1.14713254\n",
      "Iteration 248, loss = 1.14497504\n",
      "Iteration 249, loss = 1.14283095\n",
      "Iteration 250, loss = 1.14070018\n",
      "Iteration 251, loss = 1.13858264\n",
      "Iteration 252, loss = 1.13647824\n",
      "Iteration 253, loss = 1.13438690\n",
      "Iteration 254, loss = 1.13230852\n",
      "Iteration 255, loss = 1.13024301\n",
      "Iteration 256, loss = 1.12819028\n",
      "Iteration 257, loss = 1.12615023\n",
      "Iteration 258, loss = 1.12412278\n",
      "Iteration 259, loss = 1.12210782\n",
      "Iteration 260, loss = 1.12010527\n",
      "Iteration 261, loss = 1.11811502\n",
      "Iteration 262, loss = 1.11613699\n",
      "Iteration 263, loss = 1.11417107\n",
      "Iteration 264, loss = 1.11221718\n",
      "Iteration 265, loss = 1.11027521\n",
      "Iteration 266, loss = 1.10834506\n",
      "Iteration 267, loss = 1.10642665\n",
      "Iteration 268, loss = 1.10451988\n",
      "Iteration 269, loss = 1.10262463\n",
      "Iteration 270, loss = 1.10074083\n",
      "Iteration 271, loss = 1.09886837\n",
      "Iteration 272, loss = 1.09700716\n",
      "Iteration 273, loss = 1.09515708\n",
      "Iteration 274, loss = 1.09331806\n",
      "Iteration 275, loss = 1.09148999\n",
      "Iteration 276, loss = 1.08967277\n",
      "Iteration 277, loss = 1.08786631\n",
      "Iteration 278, loss = 1.08607051\n",
      "Iteration 279, loss = 1.08428526\n",
      "Iteration 280, loss = 1.08251048\n",
      "Iteration 281, loss = 1.08074607\n",
      "Iteration 282, loss = 1.07899193\n",
      "Iteration 283, loss = 1.07724796\n",
      "Iteration 284, loss = 1.07551406\n",
      "Iteration 285, loss = 1.07379015\n",
      "Iteration 286, loss = 1.07207612\n",
      "Iteration 287, loss = 1.07037189\n",
      "Iteration 288, loss = 1.06867735\n",
      "Iteration 289, loss = 1.06699240\n",
      "Iteration 290, loss = 1.06531697\n",
      "Iteration 291, loss = 1.06365095\n",
      "Iteration 292, loss = 1.06199424\n",
      "Iteration 293, loss = 1.06034676\n",
      "Iteration 294, loss = 1.05870842\n",
      "Iteration 295, loss = 1.05707911\n",
      "Iteration 296, loss = 1.05545876\n",
      "Iteration 297, loss = 1.05384726\n",
      "Iteration 298, loss = 1.05224453\n",
      "Iteration 299, loss = 1.05065048\n",
      "Iteration 300, loss = 1.04906502\n",
      "Iteration 301, loss = 1.04748805\n",
      "Iteration 302, loss = 1.04591950\n",
      "Iteration 303, loss = 1.04435927\n",
      "Iteration 304, loss = 1.04280728\n",
      "Iteration 305, loss = 1.04126344\n",
      "Iteration 306, loss = 1.03972766\n",
      "Iteration 307, loss = 1.03819985\n",
      "Iteration 308, loss = 1.03667995\n",
      "Iteration 309, loss = 1.03516785\n",
      "Iteration 310, loss = 1.03366348\n",
      "Iteration 311, loss = 1.03216675\n",
      "Iteration 312, loss = 1.03067758\n",
      "Iteration 313, loss = 1.02919590\n",
      "Iteration 314, loss = 1.02772161\n",
      "Iteration 315, loss = 1.02625465\n",
      "Iteration 316, loss = 1.02479492\n",
      "Iteration 317, loss = 1.02334235\n",
      "Iteration 318, loss = 1.02189687\n",
      "Iteration 319, loss = 1.02045840\n",
      "Iteration 320, loss = 1.01902685\n",
      "Iteration 321, loss = 1.01760216\n",
      "Iteration 322, loss = 1.01618425\n",
      "Iteration 323, loss = 1.01477304\n",
      "Iteration 324, loss = 1.01336847\n",
      "Iteration 325, loss = 1.01197045\n",
      "Iteration 326, loss = 1.01057891\n",
      "Iteration 327, loss = 1.00919380\n",
      "Iteration 328, loss = 1.00781502\n",
      "Iteration 329, loss = 1.00644252\n",
      "Iteration 330, loss = 1.00507622\n",
      "Iteration 331, loss = 1.00371606\n",
      "Iteration 332, loss = 1.00236196\n",
      "Iteration 333, loss = 1.00101387\n",
      "Iteration 334, loss = 0.99967171\n",
      "Iteration 335, loss = 0.99833542\n",
      "Iteration 336, loss = 0.99700493\n",
      "Iteration 337, loss = 0.99568017\n",
      "Iteration 338, loss = 0.99436110\n",
      "Iteration 339, loss = 0.99304763\n",
      "Iteration 340, loss = 0.99173972\n",
      "Iteration 341, loss = 0.99043729\n",
      "Iteration 342, loss = 0.98914029\n",
      "Iteration 343, loss = 0.98784866\n",
      "Iteration 344, loss = 0.98656234\n",
      "Iteration 345, loss = 0.98528126\n",
      "Iteration 346, loss = 0.98400538\n",
      "Iteration 347, loss = 0.98273463\n",
      "Iteration 348, loss = 0.98146896\n",
      "Iteration 349, loss = 0.98020831\n",
      "Iteration 350, loss = 0.97895262\n",
      "Iteration 351, loss = 0.97770185\n",
      "Iteration 352, loss = 0.97645593\n",
      "Iteration 353, loss = 0.97521482\n",
      "Iteration 354, loss = 0.97397845\n",
      "Iteration 355, loss = 0.97274679\n",
      "Iteration 356, loss = 0.97151977\n",
      "Iteration 357, loss = 0.97029735\n",
      "Iteration 358, loss = 0.96907948\n",
      "Iteration 359, loss = 0.96786610\n",
      "Iteration 360, loss = 0.96665717\n",
      "Iteration 361, loss = 0.96545264\n",
      "Iteration 362, loss = 0.96425247\n",
      "Iteration 363, loss = 0.96305659\n",
      "Iteration 364, loss = 0.96186498\n",
      "Iteration 365, loss = 0.96067758\n",
      "Iteration 366, loss = 0.95949434\n",
      "Iteration 367, loss = 0.95831523\n",
      "Iteration 368, loss = 0.95714020\n",
      "Iteration 369, loss = 0.95596920\n",
      "Iteration 370, loss = 0.95480219\n",
      "Iteration 371, loss = 0.95363913\n",
      "Iteration 372, loss = 0.95247998\n",
      "Iteration 373, loss = 0.95132469\n",
      "Iteration 374, loss = 0.95017323\n",
      "Iteration 375, loss = 0.94902556\n",
      "Iteration 376, loss = 0.94788163\n",
      "Iteration 377, loss = 0.94674141\n",
      "Iteration 378, loss = 0.94560485\n",
      "Iteration 379, loss = 0.94447192\n",
      "Iteration 380, loss = 0.94334259\n",
      "Iteration 381, loss = 0.94221681\n",
      "Iteration 382, loss = 0.94109454\n",
      "Iteration 383, loss = 0.93997576\n",
      "Iteration 384, loss = 0.93886042\n",
      "Iteration 385, loss = 0.93774850\n",
      "Iteration 386, loss = 0.93663995\n",
      "Iteration 387, loss = 0.93553474\n",
      "Iteration 388, loss = 0.93443284\n",
      "Iteration 389, loss = 0.93333421\n",
      "Iteration 390, loss = 0.93223883\n",
      "Iteration 391, loss = 0.93114665\n",
      "Iteration 392, loss = 0.93005765\n",
      "Iteration 393, loss = 0.92897180\n",
      "Iteration 394, loss = 0.92788906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 395, loss = 0.92680940\n",
      "Iteration 396, loss = 0.92573280\n",
      "Iteration 397, loss = 0.92465922\n",
      "Iteration 398, loss = 0.92358864\n",
      "Iteration 399, loss = 0.92252102\n",
      "Iteration 400, loss = 0.92145633\n",
      "Iteration 401, loss = 0.92039456\n",
      "Iteration 402, loss = 0.91933566\n",
      "Iteration 403, loss = 0.91827961\n",
      "Iteration 404, loss = 0.91722640\n",
      "Iteration 405, loss = 0.91617597\n",
      "Iteration 406, loss = 0.91512833\n",
      "Iteration 407, loss = 0.91408342\n",
      "Iteration 408, loss = 0.91304124\n",
      "Iteration 409, loss = 0.91200175\n",
      "Iteration 410, loss = 0.91096493\n",
      "Iteration 411, loss = 0.90993076\n",
      "Iteration 412, loss = 0.90889921\n",
      "Iteration 413, loss = 0.90787025\n",
      "Iteration 414, loss = 0.90684387\n",
      "Iteration 415, loss = 0.90582004\n",
      "Iteration 416, loss = 0.90479873\n",
      "Iteration 417, loss = 0.90377993\n",
      "Iteration 418, loss = 0.90276361\n",
      "Iteration 419, loss = 0.90174975\n",
      "Iteration 420, loss = 0.90073833\n",
      "Iteration 421, loss = 0.89972932\n",
      "Iteration 422, loss = 0.89872271\n",
      "Iteration 423, loss = 0.89771847\n",
      "Iteration 424, loss = 0.89671659\n",
      "Iteration 425, loss = 0.89571703\n",
      "Iteration 426, loss = 0.89471980\n",
      "Iteration 427, loss = 0.89372485\n",
      "Iteration 428, loss = 0.89273218\n",
      "Iteration 429, loss = 0.89174176\n",
      "Iteration 430, loss = 0.89075358\n",
      "Iteration 431, loss = 0.88976762\n",
      "Iteration 432, loss = 0.88878385\n",
      "Iteration 433, loss = 0.88780226\n",
      "Iteration 434, loss = 0.88682284\n",
      "Iteration 435, loss = 0.88584555\n",
      "Iteration 436, loss = 0.88487040\n",
      "Iteration 437, loss = 0.88389735\n",
      "Iteration 438, loss = 0.88292640\n",
      "Iteration 439, loss = 0.88195752\n",
      "Iteration 440, loss = 0.88099070\n",
      "Iteration 441, loss = 0.88002592\n",
      "Iteration 442, loss = 0.87906317\n",
      "Iteration 443, loss = 0.87810243\n",
      "Iteration 444, loss = 0.87714368\n",
      "Iteration 445, loss = 0.87618691\n",
      "Iteration 446, loss = 0.87523210\n",
      "Iteration 447, loss = 0.87427924\n",
      "Iteration 448, loss = 0.87332831\n",
      "Iteration 449, loss = 0.87237931\n",
      "Iteration 450, loss = 0.87143220\n",
      "Iteration 451, loss = 0.87048699\n",
      "Iteration 452, loss = 0.86954365\n",
      "Iteration 453, loss = 0.86860217\n",
      "Iteration 454, loss = 0.86766254\n",
      "Iteration 455, loss = 0.86672474\n",
      "Iteration 456, loss = 0.86578877\n",
      "Iteration 457, loss = 0.86485460\n",
      "Iteration 458, loss = 0.86392222\n",
      "Iteration 459, loss = 0.86299163\n",
      "Iteration 460, loss = 0.86206280\n",
      "Iteration 461, loss = 0.86113573\n",
      "Iteration 462, loss = 0.86021040\n",
      "Iteration 463, loss = 0.85928680\n",
      "Iteration 464, loss = 0.85836492\n",
      "Iteration 465, loss = 0.85744475\n",
      "Iteration 466, loss = 0.85652627\n",
      "Iteration 467, loss = 0.85560947\n",
      "Iteration 468, loss = 0.85469435\n",
      "Iteration 469, loss = 0.85378089\n",
      "Iteration 470, loss = 0.85286907\n",
      "Iteration 471, loss = 0.85195889\n",
      "Iteration 472, loss = 0.85105034\n",
      "Iteration 473, loss = 0.85014341\n",
      "Iteration 474, loss = 0.84923807\n",
      "Iteration 475, loss = 0.84833434\n",
      "Iteration 476, loss = 0.84743219\n",
      "Iteration 477, loss = 0.84653161\n",
      "Iteration 478, loss = 0.84563259\n",
      "Iteration 479, loss = 0.84473513\n",
      "Iteration 480, loss = 0.84383921\n",
      "Iteration 481, loss = 0.84294483\n",
      "Iteration 482, loss = 0.84205197\n",
      "Iteration 483, loss = 0.84116062\n",
      "Iteration 484, loss = 0.84027078\n",
      "Iteration 485, loss = 0.83938243\n",
      "Iteration 486, loss = 0.83849557\n",
      "Iteration 487, loss = 0.83761019\n",
      "Iteration 488, loss = 0.83672628\n",
      "Iteration 489, loss = 0.83584382\n",
      "Iteration 490, loss = 0.83496281\n",
      "Iteration 491, loss = 0.83408325\n",
      "Iteration 492, loss = 0.83320512\n",
      "Iteration 493, loss = 0.83232842\n",
      "Iteration 494, loss = 0.83145313\n",
      "Iteration 495, loss = 0.83057925\n",
      "Iteration 496, loss = 0.82970677\n",
      "Iteration 497, loss = 0.82883568\n",
      "Iteration 498, loss = 0.82796597\n",
      "Iteration 499, loss = 0.82709764\n",
      "Iteration 500, loss = 0.82623068\n",
      "Iteration 501, loss = 0.82536508\n",
      "Iteration 502, loss = 0.82450083\n",
      "Iteration 503, loss = 0.82363792\n",
      "Iteration 504, loss = 0.82277636\n",
      "Iteration 505, loss = 0.82191612\n",
      "Iteration 506, loss = 0.82105721\n",
      "Iteration 507, loss = 0.82019961\n",
      "Iteration 508, loss = 0.81934332\n",
      "Iteration 509, loss = 0.81848834\n",
      "Iteration 510, loss = 0.81763464\n",
      "Iteration 511, loss = 0.81678224\n",
      "Iteration 512, loss = 0.81593112\n",
      "Iteration 513, loss = 0.81508127\n",
      "Iteration 514, loss = 0.81423269\n",
      "Iteration 515, loss = 0.81338537\n",
      "Iteration 516, loss = 0.81253931\n",
      "Iteration 517, loss = 0.81169449\n",
      "Iteration 518, loss = 0.81085092\n",
      "Iteration 519, loss = 0.81000858\n",
      "Iteration 520, loss = 0.80916747\n",
      "Iteration 521, loss = 0.80832759\n",
      "Iteration 522, loss = 0.80748892\n",
      "Iteration 523, loss = 0.80665147\n",
      "Iteration 524, loss = 0.80581522\n",
      "Iteration 525, loss = 0.80498017\n",
      "Iteration 526, loss = 0.80414632\n",
      "Iteration 527, loss = 0.80331365\n",
      "Iteration 528, loss = 0.80248217\n",
      "Iteration 529, loss = 0.80165186\n",
      "Iteration 530, loss = 0.80082273\n",
      "Iteration 531, loss = 0.79999476\n",
      "Iteration 532, loss = 0.79916795\n",
      "Iteration 533, loss = 0.79834230\n",
      "Iteration 534, loss = 0.79751780\n",
      "Iteration 535, loss = 0.79669444\n",
      "Iteration 536, loss = 0.79587223\n",
      "Iteration 537, loss = 0.79505115\n",
      "Iteration 538, loss = 0.79423120\n",
      "Iteration 539, loss = 0.79341237\n",
      "Iteration 540, loss = 0.79259467\n",
      "Iteration 541, loss = 0.79177808\n",
      "Iteration 542, loss = 0.79096260\n",
      "Iteration 543, loss = 0.79014823\n",
      "Iteration 544, loss = 0.78933496\n",
      "Iteration 545, loss = 0.78852278\n",
      "Iteration 546, loss = 0.78771170\n",
      "Iteration 547, loss = 0.78690170\n",
      "Iteration 548, loss = 0.78609279\n",
      "Iteration 549, loss = 0.78528495\n",
      "Iteration 550, loss = 0.78447820\n",
      "Iteration 551, loss = 0.78367251\n",
      "Iteration 552, loss = 0.78286788\n",
      "Iteration 553, loss = 0.78206432\n",
      "Iteration 554, loss = 0.78126181\n",
      "Iteration 555, loss = 0.78046036\n",
      "Iteration 556, loss = 0.77965996\n",
      "Iteration 557, loss = 0.77886060\n",
      "Iteration 558, loss = 0.77806228\n",
      "Iteration 559, loss = 0.77726500\n",
      "Iteration 560, loss = 0.77646876\n",
      "Iteration 561, loss = 0.77567354\n",
      "Iteration 562, loss = 0.77487935\n",
      "Iteration 563, loss = 0.77408618\n",
      "Iteration 564, loss = 0.77329403\n",
      "Iteration 565, loss = 0.77250289\n",
      "Iteration 566, loss = 0.77171276\n",
      "Iteration 567, loss = 0.77092365\n",
      "Iteration 568, loss = 0.77013553\n",
      "Iteration 569, loss = 0.76934842\n",
      "Iteration 570, loss = 0.76856230\n",
      "Iteration 571, loss = 0.76777718\n",
      "Iteration 572, loss = 0.76699304\n",
      "Iteration 573, loss = 0.76620990\n",
      "Iteration 574, loss = 0.76542773\n",
      "Iteration 575, loss = 0.76464655\n",
      "Iteration 576, loss = 0.76386634\n",
      "Iteration 577, loss = 0.76308711\n",
      "Iteration 578, loss = 0.76230885\n",
      "Iteration 579, loss = 0.76153156\n",
      "Iteration 580, loss = 0.76075523\n",
      "Iteration 581, loss = 0.75997986\n",
      "Iteration 582, loss = 0.75920545\n",
      "Iteration 583, loss = 0.75843199\n",
      "Iteration 584, loss = 0.75765949\n",
      "Iteration 585, loss = 0.75688794\n",
      "Iteration 586, loss = 0.75611733\n",
      "Iteration 587, loss = 0.75534767\n",
      "Iteration 588, loss = 0.75457895\n",
      "Iteration 589, loss = 0.75381117\n",
      "Iteration 590, loss = 0.75304433\n",
      "Iteration 591, loss = 0.75227841\n",
      "Iteration 592, loss = 0.75151343\n",
      "Iteration 593, loss = 0.75074938\n",
      "Iteration 594, loss = 0.74998625\n",
      "Iteration 595, loss = 0.74922404\n",
      "Iteration 596, loss = 0.74846276\n",
      "Iteration 597, loss = 0.74770239\n",
      "Iteration 598, loss = 0.74694293\n",
      "Iteration 599, loss = 0.74618439\n",
      "Iteration 600, loss = 0.74542676\n",
      "Iteration 601, loss = 0.74467004\n",
      "Iteration 602, loss = 0.74391422\n",
      "Iteration 603, loss = 0.74315931\n",
      "Iteration 604, loss = 0.74240530\n",
      "Iteration 605, loss = 0.74165218\n",
      "Iteration 606, loss = 0.74089997\n",
      "Iteration 607, loss = 0.74014864\n",
      "Iteration 608, loss = 0.73939821\n",
      "Iteration 609, loss = 0.73864867\n",
      "Iteration 610, loss = 0.73790002\n",
      "Iteration 611, loss = 0.73715225\n",
      "Iteration 612, loss = 0.73640537\n",
      "Iteration 613, loss = 0.73565936\n",
      "Iteration 614, loss = 0.73491424\n",
      "Iteration 615, loss = 0.73417000\n",
      "Iteration 616, loss = 0.73342663\n",
      "Iteration 617, loss = 0.73268413\n",
      "Iteration 618, loss = 0.73194251\n",
      "Iteration 619, loss = 0.73120175\n",
      "Iteration 620, loss = 0.73046187\n",
      "Iteration 621, loss = 0.72972285\n",
      "Iteration 622, loss = 0.72898469\n",
      "Iteration 623, loss = 0.72824740\n",
      "Iteration 624, loss = 0.72751097\n",
      "Iteration 625, loss = 0.72677539\n",
      "Iteration 626, loss = 0.72604068\n",
      "Iteration 627, loss = 0.72530682\n",
      "Iteration 628, loss = 0.72457381\n",
      "Iteration 629, loss = 0.72384165\n",
      "Iteration 630, loss = 0.72311035\n",
      "Iteration 631, loss = 0.72237989\n",
      "Iteration 632, loss = 0.72165029\n",
      "Iteration 633, loss = 0.72092152\n",
      "Iteration 634, loss = 0.72019361\n",
      "Iteration 635, loss = 0.71946653\n",
      "Iteration 636, loss = 0.71874030\n",
      "Iteration 637, loss = 0.71801490\n",
      "Iteration 638, loss = 0.71729034\n",
      "Iteration 639, loss = 0.71656662\n",
      "Iteration 640, loss = 0.71584374\n",
      "Iteration 641, loss = 0.71512169\n",
      "Iteration 642, loss = 0.71440047\n",
      "Iteration 643, loss = 0.71368008\n",
      "Iteration 644, loss = 0.71296052\n",
      "Iteration 645, loss = 0.71224179\n",
      "Iteration 646, loss = 0.71152389\n",
      "Iteration 647, loss = 0.71080681\n",
      "Iteration 648, loss = 0.71009056\n",
      "Iteration 649, loss = 0.70937513\n",
      "Iteration 650, loss = 0.70866052\n",
      "Iteration 651, loss = 0.70794673\n",
      "Iteration 652, loss = 0.70723376\n",
      "Iteration 653, loss = 0.70652161\n",
      "Iteration 654, loss = 0.70581028\n",
      "Iteration 655, loss = 0.70509976\n",
      "Iteration 656, loss = 0.70439006\n",
      "Iteration 657, loss = 0.70368117\n",
      "Iteration 658, loss = 0.70297309\n",
      "Iteration 659, loss = 0.70226583\n",
      "Iteration 660, loss = 0.70155937\n",
      "Iteration 661, loss = 0.70085372\n",
      "Iteration 662, loss = 0.70014888\n",
      "Iteration 663, loss = 0.69944485\n",
      "Iteration 664, loss = 0.69874162\n",
      "Iteration 665, loss = 0.69803920\n",
      "Iteration 666, loss = 0.69733758\n",
      "Iteration 667, loss = 0.69663677\n",
      "Iteration 668, loss = 0.69593676\n",
      "Iteration 669, loss = 0.69523754\n",
      "Iteration 670, loss = 0.69453913\n",
      "Iteration 671, loss = 0.69384152\n",
      "Iteration 672, loss = 0.69314470\n",
      "Iteration 673, loss = 0.69244868\n",
      "Iteration 674, loss = 0.69175346\n",
      "Iteration 675, loss = 0.69105903\n",
      "Iteration 676, loss = 0.69036540\n",
      "Iteration 677, loss = 0.68967256\n",
      "Iteration 678, loss = 0.68898051\n",
      "Iteration 679, loss = 0.68828926\n",
      "Iteration 680, loss = 0.68759879\n",
      "Iteration 681, loss = 0.68690912\n",
      "Iteration 682, loss = 0.68622023\n",
      "Iteration 683, loss = 0.68553214\n",
      "Iteration 684, loss = 0.68484483\n",
      "Iteration 685, loss = 0.68415831\n",
      "Iteration 686, loss = 0.68347257\n",
      "Iteration 687, loss = 0.68278762\n",
      "Iteration 688, loss = 0.68210346\n",
      "Iteration 689, loss = 0.68142008\n",
      "Iteration 690, loss = 0.68073748\n",
      "Iteration 691, loss = 0.68005566\n",
      "Iteration 692, loss = 0.67937463\n",
      "Iteration 693, loss = 0.67869438\n",
      "Iteration 694, loss = 0.67801491\n",
      "Iteration 695, loss = 0.67733621\n",
      "Iteration 696, loss = 0.67665830\n",
      "Iteration 697, loss = 0.67598117\n",
      "Iteration 698, loss = 0.67530481\n",
      "Iteration 699, loss = 0.67462923\n",
      "Iteration 700, loss = 0.67395443\n",
      "Iteration 701, loss = 0.67328040\n",
      "Iteration 702, loss = 0.67260715\n",
      "Iteration 703, loss = 0.67193467\n",
      "Iteration 704, loss = 0.67126297\n",
      "Iteration 705, loss = 0.67059204\n",
      "Iteration 706, loss = 0.66992188\n",
      "Iteration 707, loss = 0.66925250\n",
      "Iteration 708, loss = 0.66858388\n",
      "Iteration 709, loss = 0.66791604\n",
      "Iteration 710, loss = 0.66724897\n",
      "Iteration 711, loss = 0.66658267\n",
      "Iteration 712, loss = 0.66591714\n",
      "Iteration 713, loss = 0.66525238\n",
      "Iteration 714, loss = 0.66458838\n",
      "Iteration 715, loss = 0.66392516\n",
      "Iteration 716, loss = 0.66326270\n",
      "Iteration 717, loss = 0.66260101\n",
      "Iteration 718, loss = 0.66194009\n",
      "Iteration 719, loss = 0.66127993\n",
      "Iteration 720, loss = 0.66062054\n",
      "Iteration 721, loss = 0.65996191\n",
      "Iteration 722, loss = 0.65930405\n",
      "Iteration 723, loss = 0.65864695\n",
      "Iteration 724, loss = 0.65799061\n",
      "Iteration 725, loss = 0.65733504\n",
      "Iteration 726, loss = 0.65668023\n",
      "Iteration 727, loss = 0.65602619\n",
      "Iteration 728, loss = 0.65537290\n",
      "Iteration 729, loss = 0.65472038\n",
      "Iteration 730, loss = 0.65406862\n",
      "Iteration 731, loss = 0.65341762\n",
      "Iteration 732, loss = 0.65276738\n",
      "Iteration 733, loss = 0.65211790\n",
      "Iteration 734, loss = 0.65146917\n",
      "Iteration 735, loss = 0.65082121\n",
      "Iteration 736, loss = 0.65017401\n",
      "Iteration 737, loss = 0.64952756\n",
      "Iteration 738, loss = 0.64888188\n",
      "Iteration 739, loss = 0.64823695\n",
      "Iteration 740, loss = 0.64759277\n",
      "Iteration 741, loss = 0.64694936\n",
      "Iteration 742, loss = 0.64630670\n",
      "Iteration 743, loss = 0.64566480\n",
      "Iteration 744, loss = 0.64502365\n",
      "Iteration 745, loss = 0.64438326\n",
      "Iteration 746, loss = 0.64374362\n",
      "Iteration 747, loss = 0.64310473\n",
      "Iteration 748, loss = 0.64246661\n",
      "Iteration 749, loss = 0.64182923\n",
      "Iteration 750, loss = 0.64119261\n",
      "Iteration 751, loss = 0.64055674\n",
      "Iteration 752, loss = 0.63992163\n",
      "Iteration 753, loss = 0.63928727\n",
      "Iteration 754, loss = 0.63865366\n",
      "Iteration 755, loss = 0.63802080\n",
      "Iteration 756, loss = 0.63738869\n",
      "Iteration 757, loss = 0.63675734\n",
      "Iteration 758, loss = 0.63612674\n",
      "Iteration 759, loss = 0.63549688\n",
      "Iteration 760, loss = 0.63486778\n",
      "Iteration 761, loss = 0.63423943\n",
      "Iteration 762, loss = 0.63361183\n",
      "Iteration 763, loss = 0.63298497\n",
      "Iteration 764, loss = 0.63235887\n",
      "Iteration 765, loss = 0.63173351\n",
      "Iteration 766, loss = 0.63110891\n",
      "Iteration 767, loss = 0.63048505\n",
      "Iteration 768, loss = 0.62986194\n",
      "Iteration 769, loss = 0.62923958\n",
      "Iteration 770, loss = 0.62861796\n",
      "Iteration 771, loss = 0.62799709\n",
      "Iteration 772, loss = 0.62737697\n",
      "Iteration 773, loss = 0.62675760\n",
      "Iteration 774, loss = 0.62613897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 775, loss = 0.62552109\n",
      "Iteration 776, loss = 0.62490395\n",
      "Iteration 777, loss = 0.62428756\n",
      "Iteration 778, loss = 0.62367192\n",
      "Iteration 779, loss = 0.62305702\n",
      "Iteration 780, loss = 0.62244286\n",
      "Iteration 781, loss = 0.62182945\n",
      "Iteration 782, loss = 0.62121678\n",
      "Iteration 783, loss = 0.62060486\n",
      "Iteration 784, loss = 0.61999368\n",
      "Iteration 785, loss = 0.61938324\n",
      "Iteration 786, loss = 0.61877355\n",
      "Iteration 787, loss = 0.61816460\n",
      "Iteration 788, loss = 0.61755639\n",
      "Iteration 789, loss = 0.61694893\n",
      "Iteration 790, loss = 0.61634220\n",
      "Iteration 791, loss = 0.61573622\n",
      "Iteration 792, loss = 0.61513098\n",
      "Iteration 793, loss = 0.61452648\n",
      "Iteration 794, loss = 0.61392272\n",
      "Iteration 795, loss = 0.61331970\n",
      "Iteration 796, loss = 0.61271743\n",
      "Iteration 797, loss = 0.61211589\n",
      "Iteration 798, loss = 0.61151509\n",
      "Iteration 799, loss = 0.61091503\n",
      "Iteration 800, loss = 0.61031571\n",
      "Iteration 801, loss = 0.60971714\n",
      "Iteration 802, loss = 0.60911930\n",
      "Iteration 803, loss = 0.60852219\n",
      "Iteration 804, loss = 0.60792583\n",
      "Iteration 805, loss = 0.60733020\n",
      "Iteration 806, loss = 0.60673532\n",
      "Iteration 807, loss = 0.60614117\n",
      "Iteration 808, loss = 0.60554775\n",
      "Iteration 809, loss = 0.60495508\n",
      "Iteration 810, loss = 0.60436314\n",
      "Iteration 811, loss = 0.60377194\n",
      "Iteration 812, loss = 0.60318147\n",
      "Iteration 813, loss = 0.60259174\n",
      "Iteration 814, loss = 0.60200274\n",
      "Iteration 815, loss = 0.60141449\n",
      "Iteration 816, loss = 0.60082696\n",
      "Iteration 817, loss = 0.60024017\n",
      "Iteration 818, loss = 0.59965412\n",
      "Iteration 819, loss = 0.59906880\n",
      "Iteration 820, loss = 0.59848421\n",
      "Iteration 821, loss = 0.59790036\n",
      "Iteration 822, loss = 0.59731724\n",
      "Iteration 823, loss = 0.59673485\n",
      "Iteration 824, loss = 0.59615320\n",
      "Iteration 825, loss = 0.59557228\n",
      "Iteration 826, loss = 0.59499209\n",
      "Iteration 827, loss = 0.59441264\n",
      "Iteration 828, loss = 0.59383391\n",
      "Iteration 829, loss = 0.59325592\n",
      "Iteration 830, loss = 0.59267866\n",
      "Iteration 831, loss = 0.59210213\n",
      "Iteration 832, loss = 0.59152633\n",
      "Iteration 833, loss = 0.59095127\n",
      "Iteration 834, loss = 0.59037693\n",
      "Iteration 835, loss = 0.58980332\n",
      "Iteration 836, loss = 0.58923044\n",
      "Iteration 837, loss = 0.58865829\n",
      "Iteration 838, loss = 0.58808687\n",
      "Iteration 839, loss = 0.58751618\n",
      "Iteration 840, loss = 0.58694622\n",
      "Iteration 841, loss = 0.58637698\n",
      "Iteration 842, loss = 0.58580847\n",
      "Iteration 843, loss = 0.58524069\n",
      "Iteration 844, loss = 0.58467364\n",
      "Iteration 845, loss = 0.58410732\n",
      "Iteration 846, loss = 0.58354172\n",
      "Iteration 847, loss = 0.58297685\n",
      "Iteration 848, loss = 0.58241270\n",
      "Iteration 849, loss = 0.58184928\n",
      "Iteration 850, loss = 0.58128658\n",
      "Iteration 851, loss = 0.58072461\n",
      "Iteration 852, loss = 0.58016337\n",
      "Iteration 853, loss = 0.57960285\n",
      "Iteration 854, loss = 0.57904305\n",
      "Iteration 855, loss = 0.57848398\n",
      "Iteration 856, loss = 0.57792563\n",
      "Iteration 857, loss = 0.57736800\n",
      "Iteration 858, loss = 0.57681110\n",
      "Iteration 859, loss = 0.57625492\n",
      "Iteration 860, loss = 0.57569946\n",
      "Iteration 861, loss = 0.57514472\n",
      "Iteration 862, loss = 0.57459071\n",
      "Iteration 863, loss = 0.57403741\n",
      "Iteration 864, loss = 0.57348484\n",
      "Iteration 865, loss = 0.57293299\n",
      "Iteration 866, loss = 0.57238186\n",
      "Iteration 867, loss = 0.57183144\n",
      "Iteration 868, loss = 0.57128175\n",
      "Iteration 869, loss = 0.57073278\n",
      "Iteration 870, loss = 0.57018452\n",
      "Iteration 871, loss = 0.56963699\n",
      "Iteration 872, loss = 0.56909017\n",
      "Iteration 873, loss = 0.56854407\n",
      "Iteration 874, loss = 0.56799868\n",
      "Iteration 875, loss = 0.56745402\n",
      "Iteration 876, loss = 0.56691007\n",
      "Iteration 877, loss = 0.56636684\n",
      "Iteration 878, loss = 0.56582432\n",
      "Iteration 879, loss = 0.56528252\n",
      "Iteration 880, loss = 0.56474143\n",
      "Iteration 881, loss = 0.56420106\n",
      "Iteration 882, loss = 0.56366140\n",
      "Iteration 883, loss = 0.56312246\n",
      "Iteration 884, loss = 0.56258423\n",
      "Iteration 885, loss = 0.56204672\n",
      "Iteration 886, loss = 0.56150991\n",
      "Iteration 887, loss = 0.56097382\n",
      "Iteration 888, loss = 0.56043845\n",
      "Iteration 889, loss = 0.55990378\n",
      "Iteration 890, loss = 0.55936983\n",
      "Iteration 891, loss = 0.55883658\n",
      "Iteration 892, loss = 0.55830405\n",
      "Iteration 893, loss = 0.55777223\n",
      "Iteration 894, loss = 0.55724112\n",
      "Iteration 895, loss = 0.55671071\n",
      "Iteration 896, loss = 0.55618102\n",
      "Iteration 897, loss = 0.55565203\n",
      "Iteration 898, loss = 0.55512375\n",
      "Iteration 899, loss = 0.55459618\n",
      "Iteration 900, loss = 0.55406932\n",
      "Iteration 901, loss = 0.55354317\n",
      "Iteration 902, loss = 0.55301772\n",
      "Iteration 903, loss = 0.55249298\n",
      "Iteration 904, loss = 0.55196894\n",
      "Iteration 905, loss = 0.55144561\n",
      "Iteration 906, loss = 0.55092298\n",
      "Iteration 907, loss = 0.55040106\n",
      "Iteration 908, loss = 0.54987984\n",
      "Iteration 909, loss = 0.54935932\n",
      "Iteration 910, loss = 0.54883951\n",
      "Iteration 911, loss = 0.54832040\n",
      "Iteration 912, loss = 0.54780200\n",
      "Iteration 913, loss = 0.54728429\n",
      "Iteration 914, loss = 0.54676729\n",
      "Iteration 915, loss = 0.54625099\n",
      "Iteration 916, loss = 0.54573538\n",
      "Iteration 917, loss = 0.54522048\n",
      "Iteration 918, loss = 0.54470628\n",
      "Iteration 919, loss = 0.54419277\n",
      "Iteration 920, loss = 0.54367997\n",
      "Iteration 921, loss = 0.54316786\n",
      "Iteration 922, loss = 0.54265645\n",
      "Iteration 923, loss = 0.54214574\n",
      "Iteration 924, loss = 0.54163573\n",
      "Iteration 925, loss = 0.54112641\n",
      "Iteration 926, loss = 0.54061778\n",
      "Iteration 927, loss = 0.54010985\n",
      "Iteration 928, loss = 0.53960262\n",
      "Iteration 929, loss = 0.53909608\n",
      "Iteration 930, loss = 0.53859024\n",
      "Iteration 931, loss = 0.53808509\n",
      "Iteration 932, loss = 0.53758063\n",
      "Iteration 933, loss = 0.53707686\n",
      "Iteration 934, loss = 0.53657379\n",
      "Iteration 935, loss = 0.53607140\n",
      "Iteration 936, loss = 0.53556971\n",
      "Iteration 937, loss = 0.53506871\n",
      "Iteration 938, loss = 0.53456840\n",
      "Iteration 939, loss = 0.53406878\n",
      "Iteration 940, loss = 0.53356984\n",
      "Iteration 941, loss = 0.53307160\n",
      "Iteration 942, loss = 0.53257404\n",
      "Iteration 943, loss = 0.53207717\n",
      "Iteration 944, loss = 0.53158099\n",
      "Iteration 945, loss = 0.53108549\n",
      "Iteration 946, loss = 0.53059068\n",
      "Iteration 947, loss = 0.53009656\n",
      "Iteration 948, loss = 0.52960311\n",
      "Iteration 949, loss = 0.52911036\n",
      "Iteration 950, loss = 0.52861829\n",
      "Iteration 951, loss = 0.52812690\n",
      "Iteration 952, loss = 0.52763619\n",
      "Iteration 953, loss = 0.52714616\n",
      "Iteration 954, loss = 0.52665682\n",
      "Iteration 955, loss = 0.52616816\n",
      "Iteration 956, loss = 0.52568018\n",
      "Iteration 957, loss = 0.52519287\n",
      "Iteration 958, loss = 0.52470625\n",
      "Iteration 959, loss = 0.52422031\n",
      "Iteration 960, loss = 0.52373504\n",
      "Iteration 961, loss = 0.52325046\n",
      "Iteration 962, loss = 0.52276655\n",
      "Iteration 963, loss = 0.52228331\n",
      "Iteration 964, loss = 0.52180075\n",
      "Iteration 965, loss = 0.52131887\n",
      "Iteration 966, loss = 0.52083766\n",
      "Iteration 967, loss = 0.52035713\n",
      "Iteration 968, loss = 0.51987727\n",
      "Iteration 969, loss = 0.51939809\n",
      "Iteration 970, loss = 0.51891957\n",
      "Iteration 971, loss = 0.51844173\n",
      "Iteration 972, loss = 0.51796456\n",
      "Iteration 973, loss = 0.51748806\n",
      "Iteration 974, loss = 0.51701223\n",
      "Iteration 975, loss = 0.51653708\n",
      "Iteration 976, loss = 0.51606259\n",
      "Iteration 977, loss = 0.51558877\n",
      "Iteration 978, loss = 0.51511561\n",
      "Iteration 979, loss = 0.51464313\n",
      "Iteration 980, loss = 0.51417131\n",
      "Iteration 981, loss = 0.51370016\n",
      "Iteration 982, loss = 0.51322967\n",
      "Iteration 983, loss = 0.51275985\n",
      "Iteration 984, loss = 0.51229069\n",
      "Iteration 985, loss = 0.51182220\n",
      "Iteration 986, loss = 0.51135437\n",
      "Iteration 987, loss = 0.51088720\n",
      "Iteration 988, loss = 0.51042069\n",
      "Iteration 989, loss = 0.50995485\n",
      "Iteration 990, loss = 0.50948966\n",
      "Iteration 991, loss = 0.50902514\n",
      "Iteration 992, loss = 0.50856128\n",
      "Iteration 993, loss = 0.50809807\n",
      "Iteration 994, loss = 0.50763552\n",
      "Iteration 995, loss = 0.50717364\n",
      "Iteration 996, loss = 0.50671240\n",
      "Iteration 997, loss = 0.50625183\n",
      "Iteration 998, loss = 0.50579191\n",
      "Iteration 999, loss = 0.50533264\n",
      "Iteration 1000, loss = 0.50487403\n",
      "Iteration 1001, loss = 0.50441608\n",
      "Iteration 1002, loss = 0.50395878\n",
      "Iteration 1003, loss = 0.50350213\n",
      "Iteration 1004, loss = 0.50304613\n",
      "Iteration 1005, loss = 0.50259078\n",
      "Iteration 1006, loss = 0.50213609\n",
      "Iteration 1007, loss = 0.50168204\n",
      "Iteration 1008, loss = 0.50122864\n",
      "Iteration 1009, loss = 0.50077590\n",
      "Iteration 1010, loss = 0.50032380\n",
      "Iteration 1011, loss = 0.49987234\n",
      "Iteration 1012, loss = 0.49942154\n",
      "Iteration 1013, loss = 0.49897138\n",
      "Iteration 1014, loss = 0.49852187\n",
      "Iteration 1015, loss = 0.49807300\n",
      "Iteration 1016, loss = 0.49762477\n",
      "Iteration 1017, loss = 0.49717719\n",
      "Iteration 1018, loss = 0.49673026\n",
      "Iteration 1019, loss = 0.49628396\n",
      "Iteration 1020, loss = 0.49583831\n",
      "Iteration 1021, loss = 0.49539329\n",
      "Iteration 1022, loss = 0.49494892\n",
      "Iteration 1023, loss = 0.49450519\n",
      "Iteration 1024, loss = 0.49406210\n",
      "Iteration 1025, loss = 0.49361964\n",
      "Iteration 1026, loss = 0.49317782\n",
      "Iteration 1027, loss = 0.49273664\n",
      "Iteration 1028, loss = 0.49229610\n",
      "Iteration 1029, loss = 0.49185619\n",
      "Iteration 1030, loss = 0.49141691\n",
      "Iteration 1031, loss = 0.49097827\n",
      "Iteration 1032, loss = 0.49054026\n",
      "Iteration 1033, loss = 0.49010289\n",
      "Iteration 1034, loss = 0.48966615\n",
      "Iteration 1035, loss = 0.48923004\n",
      "Iteration 1036, loss = 0.48879456\n",
      "Iteration 1037, loss = 0.48835971\n",
      "Iteration 1038, loss = 0.48792549\n",
      "Iteration 1039, loss = 0.48749190\n",
      "Iteration 1040, loss = 0.48705893\n",
      "Iteration 1041, loss = 0.48662660\n",
      "Iteration 1042, loss = 0.48619489\n",
      "Iteration 1043, loss = 0.48576381\n",
      "Iteration 1044, loss = 0.48533335\n",
      "Iteration 1045, loss = 0.48490351\n",
      "Iteration 1046, loss = 0.48447430\n",
      "Iteration 1047, loss = 0.48404572\n",
      "Iteration 1048, loss = 0.48361775\n",
      "Iteration 1049, loss = 0.48319041\n",
      "Iteration 1050, loss = 0.48276369\n",
      "Iteration 1051, loss = 0.48233759\n",
      "Iteration 1052, loss = 0.48191211\n",
      "Iteration 1053, loss = 0.48148725\n",
      "Iteration 1054, loss = 0.48106300\n",
      "Iteration 1055, loss = 0.48063938\n",
      "Iteration 1056, loss = 0.48021637\n",
      "Iteration 1057, loss = 0.47979397\n",
      "Iteration 1058, loss = 0.47937219\n",
      "Iteration 1059, loss = 0.47895103\n",
      "Iteration 1060, loss = 0.47853048\n",
      "Iteration 1061, loss = 0.47811054\n",
      "Iteration 1062, loss = 0.47769122\n",
      "Iteration 1063, loss = 0.47727251\n",
      "Iteration 1064, loss = 0.47685441\n",
      "Iteration 1065, loss = 0.47643692\n",
      "Iteration 1066, loss = 0.47602003\n",
      "Iteration 1067, loss = 0.47560376\n",
      "Iteration 1068, loss = 0.47518810\n",
      "Iteration 1069, loss = 0.47477304\n",
      "Iteration 1070, loss = 0.47435859\n",
      "Iteration 1071, loss = 0.47394475\n",
      "Iteration 1072, loss = 0.47353151\n",
      "Iteration 1073, loss = 0.47311887\n",
      "Iteration 1074, loss = 0.47270684\n",
      "Iteration 1075, loss = 0.47229542\n",
      "Iteration 1076, loss = 0.47188459\n",
      "Iteration 1077, loss = 0.47147437\n",
      "Iteration 1078, loss = 0.47106475\n",
      "Iteration 1079, loss = 0.47065573\n",
      "Iteration 1080, loss = 0.47024730\n",
      "Iteration 1081, loss = 0.46983948\n",
      "Iteration 1082, loss = 0.46943225\n",
      "Iteration 1083, loss = 0.46902563\n",
      "Iteration 1084, loss = 0.46861959\n",
      "Iteration 1085, loss = 0.46821416\n",
      "Iteration 1086, loss = 0.46780932\n",
      "Iteration 1087, loss = 0.46740507\n",
      "Iteration 1088, loss = 0.46700142\n",
      "Iteration 1089, loss = 0.46659835\n",
      "Iteration 1090, loss = 0.46619589\n",
      "Iteration 1091, loss = 0.46579401\n",
      "Iteration 1092, loss = 0.46539272\n",
      "Iteration 1093, loss = 0.46499202\n",
      "Iteration 1094, loss = 0.46459192\n",
      "Iteration 1095, loss = 0.46419240\n",
      "Iteration 1096, loss = 0.46379347\n",
      "Iteration 1097, loss = 0.46339512\n",
      "Iteration 1098, loss = 0.46299736\n",
      "Iteration 1099, loss = 0.46260019\n",
      "Iteration 1100, loss = 0.46220360\n",
      "Iteration 1101, loss = 0.46180759\n",
      "Iteration 1102, loss = 0.46141217\n",
      "Iteration 1103, loss = 0.46101733\n",
      "Iteration 1104, loss = 0.46062308\n",
      "Iteration 1105, loss = 0.46022940\n",
      "Iteration 1106, loss = 0.45983630\n",
      "Iteration 1107, loss = 0.45944378\n",
      "Iteration 1108, loss = 0.45905184\n",
      "Iteration 1109, loss = 0.45866048\n",
      "Iteration 1110, loss = 0.45826970\n",
      "Iteration 1111, loss = 0.45787949\n",
      "Iteration 1112, loss = 0.45748986\n",
      "Iteration 1113, loss = 0.45710080\n",
      "Iteration 1114, loss = 0.45671232\n",
      "Iteration 1115, loss = 0.45632441\n",
      "Iteration 1116, loss = 0.45593707\n",
      "Iteration 1117, loss = 0.45555031\n",
      "Iteration 1118, loss = 0.45516411\n",
      "Iteration 1119, loss = 0.45477849\n",
      "Iteration 1120, loss = 0.45439343\n",
      "Iteration 1121, loss = 0.45400895\n",
      "Iteration 1122, loss = 0.45362503\n",
      "Iteration 1123, loss = 0.45324168\n",
      "Iteration 1124, loss = 0.45285890\n",
      "Iteration 1125, loss = 0.45247668\n",
      "Iteration 1126, loss = 0.45209502\n",
      "Iteration 1127, loss = 0.45171393\n",
      "Iteration 1128, loss = 0.45133341\n",
      "Iteration 1129, loss = 0.45095345\n",
      "Iteration 1130, loss = 0.45057405\n",
      "Iteration 1131, loss = 0.45019521\n",
      "Iteration 1132, loss = 0.44981693\n",
      "Iteration 1133, loss = 0.44943921\n",
      "Iteration 1134, loss = 0.44906205\n",
      "Iteration 1135, loss = 0.44868544\n",
      "Iteration 1136, loss = 0.44830940\n",
      "Iteration 1137, loss = 0.44793391\n",
      "Iteration 1138, loss = 0.44755898\n",
      "Iteration 1139, loss = 0.44718460\n",
      "Iteration 1140, loss = 0.44681077\n",
      "Iteration 1141, loss = 0.44643750\n",
      "Iteration 1142, loss = 0.44606479\n",
      "Iteration 1143, loss = 0.44569262\n",
      "Iteration 1144, loss = 0.44532101\n",
      "Iteration 1145, loss = 0.44494995\n",
      "Iteration 1146, loss = 0.44457943\n",
      "Iteration 1147, loss = 0.44420947\n",
      "Iteration 1148, loss = 0.44384005\n",
      "Iteration 1149, loss = 0.44347118\n",
      "Iteration 1150, loss = 0.44310286\n",
      "Iteration 1151, loss = 0.44273508\n",
      "Iteration 1152, loss = 0.44236785\n",
      "Iteration 1153, loss = 0.44200117\n",
      "Iteration 1154, loss = 0.44163502\n",
      "Iteration 1155, loss = 0.44126942\n",
      "Iteration 1156, loss = 0.44090436\n",
      "Iteration 1157, loss = 0.44053985\n",
      "Iteration 1158, loss = 0.44017587\n",
      "Iteration 1159, loss = 0.43981243\n",
      "Iteration 1160, loss = 0.43944953\n",
      "Iteration 1161, loss = 0.43908718\n",
      "Iteration 1162, loss = 0.43872535\n",
      "Iteration 1163, loss = 0.43836407\n",
      "Iteration 1164, loss = 0.43800332\n",
      "Iteration 1165, loss = 0.43764310\n",
      "Iteration 1166, loss = 0.43728342\n",
      "Iteration 1167, loss = 0.43692428\n",
      "Iteration 1168, loss = 0.43656567\n",
      "Iteration 1169, loss = 0.43620758\n",
      "Iteration 1170, loss = 0.43585003\n",
      "Iteration 1171, loss = 0.43549302\n",
      "Iteration 1172, loss = 0.43513653\n",
      "Iteration 1173, loss = 0.43478057\n",
      "Iteration 1174, loss = 0.43442513\n",
      "Iteration 1175, loss = 0.43407023\n",
      "Iteration 1176, loss = 0.43371585\n",
      "Iteration 1177, loss = 0.43336200\n",
      "Iteration 1178, loss = 0.43300867\n",
      "Iteration 1179, loss = 0.43265587\n",
      "Iteration 1180, loss = 0.43230359\n",
      "Iteration 1181, loss = 0.43195184\n",
      "Iteration 1182, loss = 0.43160060\n",
      "Iteration 1183, loss = 0.43124989\n",
      "Iteration 1184, loss = 0.43089970\n",
      "Iteration 1185, loss = 0.43055003\n",
      "Iteration 1186, loss = 0.43020088\n",
      "Iteration 1187, loss = 0.42985224\n",
      "Iteration 1188, loss = 0.42950413\n",
      "Iteration 1189, loss = 0.42915653\n",
      "Iteration 1190, loss = 0.42880945\n",
      "Iteration 1191, loss = 0.42846288\n",
      "Iteration 1192, loss = 0.42811682\n",
      "Iteration 1193, loss = 0.42777128\n",
      "Iteration 1194, loss = 0.42742626\n",
      "Iteration 1195, loss = 0.42708174\n",
      "Iteration 1196, loss = 0.42673774\n",
      "Iteration 1197, loss = 0.42639425\n",
      "Iteration 1198, loss = 0.42605127\n",
      "Iteration 1199, loss = 0.42570879\n",
      "Iteration 1200, loss = 0.42536683\n",
      "Iteration 1201, loss = 0.42502537\n",
      "Iteration 1202, loss = 0.42468442\n",
      "Iteration 1203, loss = 0.42434398\n",
      "Iteration 1204, loss = 0.42400404\n",
      "Iteration 1205, loss = 0.42366461\n",
      "Iteration 1206, loss = 0.42332568\n",
      "Iteration 1207, loss = 0.42298725\n",
      "Iteration 1208, loss = 0.42264932\n",
      "Iteration 1209, loss = 0.42231190\n",
      "Iteration 1210, loss = 0.42197498\n",
      "Iteration 1211, loss = 0.42163856\n",
      "Iteration 1212, loss = 0.42130264\n",
      "Iteration 1213, loss = 0.42096721\n",
      "Iteration 1214, loss = 0.42063229\n",
      "Iteration 1215, loss = 0.42029786\n",
      "Iteration 1216, loss = 0.41996392\n",
      "Iteration 1217, loss = 0.41963049\n",
      "Iteration 1218, loss = 0.41929754\n",
      "Iteration 1219, loss = 0.41896510\n",
      "Iteration 1220, loss = 0.41863314\n",
      "Iteration 1221, loss = 0.41830168\n",
      "Iteration 1222, loss = 0.41797071\n",
      "Iteration 1223, loss = 0.41764023\n",
      "Iteration 1224, loss = 0.41731024\n",
      "Iteration 1225, loss = 0.41698074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1226, loss = 0.41665173\n",
      "Iteration 1227, loss = 0.41632320\n",
      "Iteration 1228, loss = 0.41599517\n",
      "Iteration 1229, loss = 0.41566762\n",
      "Iteration 1230, loss = 0.41534056\n",
      "Iteration 1231, loss = 0.41501398\n",
      "Iteration 1232, loss = 0.41468788\n",
      "Iteration 1233, loss = 0.41436227\n",
      "Iteration 1234, loss = 0.41403715\n",
      "Iteration 1235, loss = 0.41371250\n",
      "Iteration 1236, loss = 0.41338834\n",
      "Iteration 1237, loss = 0.41306465\n",
      "Iteration 1238, loss = 0.41274145\n",
      "Iteration 1239, loss = 0.41241872\n",
      "Iteration 1240, loss = 0.41209648\n",
      "Iteration 1241, loss = 0.41177471\n",
      "Iteration 1242, loss = 0.41145341\n",
      "Iteration 1243, loss = 0.41113260\n",
      "Iteration 1244, loss = 0.41081226\n",
      "Iteration 1245, loss = 0.41049239\n",
      "Iteration 1246, loss = 0.41017300\n",
      "Iteration 1247, loss = 0.40985408\n",
      "Iteration 1248, loss = 0.40953563\n",
      "Iteration 1249, loss = 0.40921766\n",
      "Iteration 1250, loss = 0.40890015\n",
      "Iteration 1251, loss = 0.40858312\n",
      "Iteration 1252, loss = 0.40826655\n",
      "Iteration 1253, loss = 0.40795046\n",
      "Iteration 1254, loss = 0.40763483\n",
      "Iteration 1255, loss = 0.40731967\n",
      "Iteration 1256, loss = 0.40700497\n",
      "Iteration 1257, loss = 0.40669074\n",
      "Iteration 1258, loss = 0.40637698\n",
      "Iteration 1259, loss = 0.40606368\n",
      "Iteration 1260, loss = 0.40575085\n",
      "Iteration 1261, loss = 0.40543847\n",
      "Iteration 1262, loss = 0.40512656\n",
      "Iteration 1263, loss = 0.40481511\n",
      "Iteration 1264, loss = 0.40450412\n",
      "Iteration 1265, loss = 0.40419359\n",
      "Iteration 1266, loss = 0.40388352\n",
      "Iteration 1267, loss = 0.40357391\n",
      "Iteration 1268, loss = 0.40326476\n",
      "Iteration 1269, loss = 0.40295606\n",
      "Iteration 1270, loss = 0.40264782\n",
      "Iteration 1271, loss = 0.40234003\n",
      "Iteration 1272, loss = 0.40203270\n",
      "Iteration 1273, loss = 0.40172583\n",
      "Iteration 1274, loss = 0.40141940\n",
      "Iteration 1275, loss = 0.40111343\n",
      "Iteration 1276, loss = 0.40080791\n",
      "Iteration 1277, loss = 0.40050284\n",
      "Iteration 1278, loss = 0.40019823\n",
      "Iteration 1279, loss = 0.39989406\n",
      "Iteration 1280, loss = 0.39959034\n",
      "Iteration 1281, loss = 0.39928707\n",
      "Iteration 1282, loss = 0.39898425\n",
      "Iteration 1283, loss = 0.39868187\n",
      "Iteration 1284, loss = 0.39837994\n",
      "Iteration 1285, loss = 0.39807846\n",
      "Iteration 1286, loss = 0.39777742\n",
      "Iteration 1287, loss = 0.39747682\n",
      "Iteration 1288, loss = 0.39717667\n",
      "Iteration 1289, loss = 0.39687696\n",
      "Iteration 1290, loss = 0.39657769\n",
      "Iteration 1291, loss = 0.39627886\n",
      "Iteration 1292, loss = 0.39598048\n",
      "Iteration 1293, loss = 0.39568253\n",
      "Iteration 1294, loss = 0.39538502\n",
      "Iteration 1295, loss = 0.39508795\n",
      "Iteration 1296, loss = 0.39479132\n",
      "Iteration 1297, loss = 0.39449512\n",
      "Iteration 1298, loss = 0.39419936\n",
      "Iteration 1299, loss = 0.39390404\n",
      "Iteration 1300, loss = 0.39360915\n",
      "Iteration 1301, loss = 0.39331470\n",
      "Iteration 1302, loss = 0.39302067\n",
      "Iteration 1303, loss = 0.39272708\n",
      "Iteration 1304, loss = 0.39243393\n",
      "Iteration 1305, loss = 0.39214120\n",
      "Iteration 1306, loss = 0.39184890\n",
      "Iteration 1307, loss = 0.39155704\n",
      "Iteration 1308, loss = 0.39126560\n",
      "Iteration 1309, loss = 0.39097459\n",
      "Iteration 1310, loss = 0.39068401\n",
      "Iteration 1311, loss = 0.39039386\n",
      "Iteration 1312, loss = 0.39010413\n",
      "Iteration 1313, loss = 0.38981483\n",
      "Iteration 1314, loss = 0.38952595\n",
      "Iteration 1315, loss = 0.38923750\n",
      "Iteration 1316, loss = 0.38894947\n",
      "Iteration 1317, loss = 0.38866186\n",
      "Iteration 1318, loss = 0.38837468\n",
      "Iteration 1319, loss = 0.38808792\n",
      "Iteration 1320, loss = 0.38780157\n",
      "Iteration 1321, loss = 0.38751565\n",
      "Iteration 1322, loss = 0.38723015\n",
      "Iteration 1323, loss = 0.38694506\n",
      "Iteration 1324, loss = 0.38666040\n",
      "Iteration 1325, loss = 0.38637615\n",
      "Iteration 1326, loss = 0.38609232\n",
      "Iteration 1327, loss = 0.38580890\n",
      "Iteration 1328, loss = 0.38552590\n",
      "Iteration 1329, loss = 0.38524331\n",
      "Iteration 1330, loss = 0.38496114\n",
      "Iteration 1331, loss = 0.38467938\n",
      "Iteration 1332, loss = 0.38439803\n",
      "Iteration 1333, loss = 0.38411710\n",
      "Iteration 1334, loss = 0.38383657\n",
      "Iteration 1335, loss = 0.38355646\n",
      "Iteration 1336, loss = 0.38327675\n",
      "Iteration 1337, loss = 0.38299746\n",
      "Iteration 1338, loss = 0.38271857\n",
      "Iteration 1339, loss = 0.38244009\n",
      "Iteration 1340, loss = 0.38216202\n",
      "Iteration 1341, loss = 0.38188436\n",
      "Iteration 1342, loss = 0.38160710\n",
      "Iteration 1343, loss = 0.38133024\n",
      "Iteration 1344, loss = 0.38105379\n",
      "Iteration 1345, loss = 0.38077774\n",
      "Iteration 1346, loss = 0.38050210\n",
      "Iteration 1347, loss = 0.38022686\n",
      "Iteration 1348, loss = 0.37995202\n",
      "Iteration 1349, loss = 0.37967758\n",
      "Iteration 1350, loss = 0.37940354\n",
      "Iteration 1351, loss = 0.37912991\n",
      "Iteration 1352, loss = 0.37885667\n",
      "Iteration 1353, loss = 0.37858383\n",
      "Iteration 1354, loss = 0.37831138\n",
      "Iteration 1355, loss = 0.37803934\n",
      "Iteration 1356, loss = 0.37776769\n",
      "Iteration 1357, loss = 0.37749643\n",
      "Iteration 1358, loss = 0.37722557\n",
      "Iteration 1359, loss = 0.37695511\n",
      "Iteration 1360, loss = 0.37668504\n",
      "Iteration 1361, loss = 0.37641536\n",
      "Iteration 1362, loss = 0.37614607\n",
      "Iteration 1363, loss = 0.37587718\n",
      "Iteration 1364, loss = 0.37560868\n",
      "Iteration 1365, loss = 0.37534056\n",
      "Iteration 1366, loss = 0.37507284\n",
      "Iteration 1367, loss = 0.37480551\n",
      "Iteration 1368, loss = 0.37453856\n",
      "Iteration 1369, loss = 0.37427201\n",
      "Iteration 1370, loss = 0.37400584\n",
      "Iteration 1371, loss = 0.37374006\n",
      "Iteration 1372, loss = 0.37347466\n",
      "Iteration 1373, loss = 0.37320965\n",
      "Iteration 1374, loss = 0.37294502\n",
      "Iteration 1375, loss = 0.37268078\n",
      "Iteration 1376, loss = 0.37241692\n",
      "Iteration 1377, loss = 0.37215344\n",
      "Iteration 1378, loss = 0.37189034\n",
      "Iteration 1379, loss = 0.37162763\n",
      "Iteration 1380, loss = 0.37136530\n",
      "Iteration 1381, loss = 0.37110334\n",
      "Iteration 1382, loss = 0.37084177\n",
      "Iteration 1383, loss = 0.37058057\n",
      "Iteration 1384, loss = 0.37031976\n",
      "Iteration 1385, loss = 0.37005932\n",
      "Iteration 1386, loss = 0.36979926\n",
      "Iteration 1387, loss = 0.36953957\n",
      "Iteration 1388, loss = 0.36928026\n",
      "Iteration 1389, loss = 0.36902132\n",
      "Iteration 1390, loss = 0.36876276\n",
      "Iteration 1391, loss = 0.36850458\n",
      "Iteration 1392, loss = 0.36824676\n",
      "Iteration 1393, loss = 0.36798932\n",
      "Iteration 1394, loss = 0.36773225\n",
      "Iteration 1395, loss = 0.36747555\n",
      "Iteration 1396, loss = 0.36721923\n",
      "Iteration 1397, loss = 0.36696327\n",
      "Iteration 1398, loss = 0.36670768\n",
      "Iteration 1399, loss = 0.36645246\n",
      "Iteration 1400, loss = 0.36619761\n",
      "Iteration 1401, loss = 0.36594312\n",
      "Iteration 1402, loss = 0.36568901\n",
      "Iteration 1403, loss = 0.36543526\n",
      "Iteration 1404, loss = 0.36518187\n",
      "Iteration 1405, loss = 0.36492885\n",
      "Iteration 1406, loss = 0.36467620\n",
      "Iteration 1407, loss = 0.36442390\n",
      "Iteration 1408, loss = 0.36417197\n",
      "Iteration 1409, loss = 0.36392041\n",
      "Iteration 1410, loss = 0.36366920\n",
      "Iteration 1411, loss = 0.36341836\n",
      "Iteration 1412, loss = 0.36316788\n",
      "Iteration 1413, loss = 0.36291776\n",
      "Iteration 1414, loss = 0.36266799\n",
      "Iteration 1415, loss = 0.36241859\n",
      "Iteration 1416, loss = 0.36216954\n",
      "Iteration 1417, loss = 0.36192086\n",
      "Iteration 1418, loss = 0.36167252\n",
      "Iteration 1419, loss = 0.36142455\n",
      "Iteration 1420, loss = 0.36117693\n",
      "Iteration 1421, loss = 0.36092967\n",
      "Iteration 1422, loss = 0.36068276\n",
      "Iteration 1423, loss = 0.36043620\n",
      "Iteration 1424, loss = 0.36019000\n",
      "Iteration 1425, loss = 0.35994415\n",
      "Iteration 1426, loss = 0.35969866\n",
      "Iteration 1427, loss = 0.35945351\n",
      "Iteration 1428, loss = 0.35920872\n",
      "Iteration 1429, loss = 0.35896428\n",
      "Iteration 1430, loss = 0.35872018\n",
      "Iteration 1431, loss = 0.35847644\n",
      "Iteration 1432, loss = 0.35823304\n",
      "Iteration 1433, loss = 0.35799000\n",
      "Iteration 1434, loss = 0.35774730\n",
      "Iteration 1435, loss = 0.35750494\n",
      "Iteration 1436, loss = 0.35726294\n",
      "Iteration 1437, loss = 0.35702128\n",
      "Iteration 1438, loss = 0.35677996\n",
      "Iteration 1439, loss = 0.35653899\n",
      "Iteration 1440, loss = 0.35629836\n",
      "Iteration 1441, loss = 0.35605808\n",
      "Iteration 1442, loss = 0.35581813\n",
      "Iteration 1443, loss = 0.35557854\n",
      "Iteration 1444, loss = 0.35533928\n",
      "Iteration 1445, loss = 0.35510036\n",
      "Iteration 1446, loss = 0.35486178\n",
      "Iteration 1447, loss = 0.35462355\n",
      "Iteration 1448, loss = 0.35438565\n",
      "Iteration 1449, loss = 0.35414809\n",
      "Iteration 1450, loss = 0.35391087\n",
      "Iteration 1451, loss = 0.35367399\n",
      "Iteration 1452, loss = 0.35343744\n",
      "Iteration 1453, loss = 0.35320123\n",
      "Iteration 1454, loss = 0.35296536\n",
      "Iteration 1455, loss = 0.35272982\n",
      "Iteration 1456, loss = 0.35249462\n",
      "Iteration 1457, loss = 0.35225975\n",
      "Iteration 1458, loss = 0.35202521\n",
      "Iteration 1459, loss = 0.35179101\n",
      "Iteration 1460, loss = 0.35155714\n",
      "Iteration 1461, loss = 0.35132360\n",
      "Iteration 1462, loss = 0.35109039\n",
      "Iteration 1463, loss = 0.35085752\n",
      "Iteration 1464, loss = 0.35062497\n",
      "Iteration 1465, loss = 0.35039275\n",
      "Iteration 1466, loss = 0.35016086\n",
      "Iteration 1467, loss = 0.34992930\n",
      "Iteration 1468, loss = 0.34969807\n",
      "Iteration 1469, loss = 0.34946717\n",
      "Iteration 1470, loss = 0.34923659\n",
      "Iteration 1471, loss = 0.34900634\n",
      "Iteration 1472, loss = 0.34877641\n",
      "Iteration 1473, loss = 0.34854681\n",
      "Iteration 1474, loss = 0.34831753\n",
      "Iteration 1475, loss = 0.34808858\n",
      "Iteration 1476, loss = 0.34785995\n",
      "Iteration 1477, loss = 0.34763164\n",
      "Iteration 1478, loss = 0.34740366\n",
      "Iteration 1479, loss = 0.34717600\n",
      "Iteration 1480, loss = 0.34694866\n",
      "Iteration 1481, loss = 0.34672164\n",
      "Iteration 1482, loss = 0.34649494\n",
      "Iteration 1483, loss = 0.34626855\n",
      "Iteration 1484, loss = 0.34604249\n",
      "Iteration 1485, loss = 0.34581675\n",
      "Iteration 1486, loss = 0.34559132\n",
      "Iteration 1487, loss = 0.34536622\n",
      "Iteration 1488, loss = 0.34514142\n",
      "Iteration 1489, loss = 0.34491695\n",
      "Iteration 1490, loss = 0.34469279\n",
      "Iteration 1491, loss = 0.34446895\n",
      "Iteration 1492, loss = 0.34424542\n",
      "Iteration 1493, loss = 0.34402220\n",
      "Iteration 1494, loss = 0.34379930\n",
      "Iteration 1495, loss = 0.34357671\n",
      "Iteration 1496, loss = 0.34335443\n",
      "Iteration 1497, loss = 0.34313247\n",
      "Iteration 1498, loss = 0.34291082\n",
      "Iteration 1499, loss = 0.34268948\n",
      "Iteration 1500, loss = 0.34246844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1500, momentum=0.95,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_iris.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluando el desempeño de entrenamiento de la Red Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_iris.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evaluando el desempeño de la Red Iris en el dataset de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_iris.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dibujando la función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cost_function = red_iris.loss_curve_\n",
    "plt.figure()\n",
    "plt.plot(cost_function)\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
